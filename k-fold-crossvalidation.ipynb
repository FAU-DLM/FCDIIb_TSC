{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold-Crossvalidation with a VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "- get validation process straight\n",
    "- hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.zip', 'base_dir', 'test', 'train_labels.csv.zip', 'train.zip', 'sample_submission.csv.zip', 'train_labels.csv', 'train']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "print(os.listdir(\"/home/Deep_Learner/work/local/histopathologic-cancer-detection\"))\n",
    "\n",
    "from glob import glob \n",
    "from skimage.io import imread\n",
    "import gc\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras import applications, regularizers, optimizers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#fast plot of training history\n",
    "def plot_history(history, modelname):\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n",
    "    axs[0].plot(hist_df.val_acc, lw=5, label='Validation Accuracy')\n",
    "    axs[0].plot(hist_df.acc, lw=5, label='Training Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].grid()\n",
    "    axs[0].legend(loc=0)\n",
    "    axs[1].plot(hist_df.val_loss, lw=5, label='Validation MLogLoss')\n",
    "    axs[1].plot(hist_df.loss, lw=5, label='Training MLogLoss')\n",
    "    axs[1].set_ylabel('MLogLoss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].grid()\n",
    "    axs[1].legend(loc=0)\n",
    "    fig.savefig('History_{}.png' .format(modelname), dpi=300)\n",
    "    plt.show();\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc#plotting the receiver operating characteristics --> evaluate performance cutting point vice\n",
    "def plot_roc(label, predictions): #IDEA: set diffrent cutting point based on ROC for ensembling   \n",
    "    roc_auc_score(label, predictions)\n",
    "    print('The ROC-Score is: {}' .format(roc_auc_score))\n",
    "\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(label, predictions)\n",
    "    auc_keras = auc(fpr_keras, tpr_keras)\n",
    "    #print(auc_keras)\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve: {}' .format(auc_keras))\n",
    "    plt.legend(loc='best')\n",
    "    #fig.savefig('ROC-Curve.png', dpi=300) #saving PLOT \n",
    "    plt.show()\n",
    "    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "# Source: Scikit Learn website\n",
    "# http://scikit-learn.org/stable/auto_examples/\n",
    "# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n",
    "# selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "from scipy.misc import imread\n",
    "#list of tiffs to array to plot_correct / incorrect-images + rounding predictions to compare to labels\n",
    "def prep_im_label (val, y_pred):\n",
    "    vals = []\n",
    "    for i in val:\n",
    "        vals.append(imread(i))\n",
    "        #q = q+1\n",
    "        #print(q)\n",
    "        #print(i)\n",
    "    vals = np.asarray(vals)    \n",
    "    print(vals.shape)\n",
    "    y_pred = np.round(y_pred)\n",
    "    return(vals, y_pred)\n",
    "\n",
    "#plotting correctly classified images: https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python\n",
    "def plot_correct(vals, y_pred, y_label, modelname):\n",
    "    correct = np.where(y_pred==y_label)[0]\n",
    "    print (\"Found %d correct labels\" % len(correct))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.suptitle('Correct Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, correct in enumerate(correct[:9]):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(vals[correct])\n",
    "        ax.set_title(\"Predicted {}, Class {}\".format(y_pred[correct], y_label[correct]), fontsize=10)\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "    #fig.savefig('Correct_Images_{}.png' .format(modelname), dpi=100) #saving PLOT \n",
    "\n",
    "#Plotting incorrectly classified\n",
    "def plot_incorrect(vals, y_pred, y_label, modelname):\n",
    "    incorrect = np.where(y_pred!=y_label)[0]\n",
    "    print (\"Found %d incorrect labels\" % len(incorrect))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.suptitle('Incorrect Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, incorrect in enumerate(incorrect[:9]):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(vals[incorrect])\n",
    "        ax.set_title(\"Predicted {}, Class {}\".format(y_pred[incorrect], y_label[incorrect]), fontsize=10)\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "    #fig.savefig('Incorrect_Images._{}.png' .format(modelname), dpi=100) #saving PLOT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-16 for heatmap generation \n",
    "img_width_VGG, img_height_VGG = 96, 96 #VGG\n",
    "modelname = 'VGG'\n",
    "\n",
    "from time import time\n",
    "from keras.applications.nasnet import NASNetMobile, NASNetLarge, preprocess_input\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint, Callback, LambdaCallback\n",
    "from keras.layers import Convolution1D, concatenate, SpatialDropout1D, GlobalMaxPool1D, GlobalAvgPool1D, Embedding, \\\n",
    "    Conv2D, SeparableConv1D, Add, BatchNormalization, Activation, GlobalAveragePooling2D, LeakyReLU, Flatten\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, \\\n",
    "    Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n",
    "from keras.losses import mae, sparse_categorical_crossentropy, binary_crossentropy\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "def get_model_classif_VGG_base_nottrainable():\n",
    "    base_model_VGG = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_width_VGG, img_height_VGG, 3))\n",
    "    print('Model loaded.')\n",
    "    \n",
    "    for layer in base_model_VGG.layers:\n",
    "        layer.trainable = False\n",
    "    #    print(\"trainable:\", layer.name)\n",
    "    \n",
    "    x = base_model_VGG.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = (Dense(256, activation='relu'))(x)\n",
    "    x = (Dropout(0.5)) (x)\n",
    "    predictions = (Dense(1, activation='sigmoid'))(x)\n",
    "    model_VGG = Model(inputs=base_model_VGG.input, outputs=predictions)\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "    model_VGG.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    model_VGG.summary()\n",
    "    return model_VGG\n",
    "\n",
    "def get_model_classif_VGG_base_trainable():\n",
    "    base_model_VGG = applications.VGG16(weights=None, include_top=False, input_shape=(img_width_VGG, img_height_VGG, 3))\n",
    "    print('Model loaded.')\n",
    "    \n",
    "    for layer in base_model_VGG.layers:\n",
    "        layer.trainable = True\n",
    "    #    print(\"trainable:\", layer.name)\n",
    "    \n",
    "    x = base_model_VGG.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = (Dense(256, activation='relu'))(x)\n",
    "    x = (Dropout(0.5)) (x)\n",
    "    predictions = (Dense(1, activation='sigmoid'))(x)\n",
    "    model_VGG = Model(inputs=base_model_VGG.input, outputs=predictions)\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.0001)\n",
    "    model_VGG.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    model_VGG.summary()\n",
    "    return model_VGG\n",
    "\n",
    "def get_callbacks(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=1, mode='auto')\n",
    "    tensorboard_callback = TensorBoard(log_dir=\"logs_NasnetM/{}\".format(time()), \n",
    "            histogram_freq=0, \n",
    "            write_graph=True, write_grads=False, write_images=True)\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    return [earlystopping, ReduceLR, tensorboard_callback, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#visualize Augmentation from custom datagen!\\ndef looking_at_augmentation (data_generator, batchsize):\\n    aug_gen = datagen.flow(X, y, batch_size = batchsize)\\n\\n    im, label = next(aug_gen)  \\n    im = (im - np.min(im))/np.ptp(im) # to normalize all images --> matplotlib only takes pos. values between 0..1 / 0..255 \\n    imgs = list(im)\\n    labels = list(label)\\n    \\n    fig, ax = plt.subplots(ncols=3, nrows=3)\\n    fig.subplots_adjust(hspace=0.5)\\n    plt.suptitle(\\'Augmented Images\\', fontsize=16)\\n    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor=\\'w\\', edgecolor=\\'k\\')\\n\\n    for ax in ax.flatten():\\n        ax.axis(\\'off\\')\\n    \\n    i = 0 \\n    for im, l  in zip(imgs, labels):\\n        ax = fig.add_subplot(3,3,i+1)\\n        ax.imshow(im)\\n        i = i+1\\n        ax.set_title(\"Label: {}\".format(l))\\n        fig.set_figheight(8)\\n        fig.set_figwidth(8)\\n        if i == batchsize:\\n            break\\n\\n    #fig.tight_layout()\\n    #fig.savefig(\\'Augmented-Images.png\\', dpi=100)\\n\\nlooking_at_augmentation(datagen, batchsize=9)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#visualize Augmentation from custom datagen!\n",
    "def looking_at_augmentation (data_generator, batchsize):\n",
    "    aug_gen = datagen.flow(X, y, batch_size = batchsize)\n",
    "\n",
    "    im, label = next(aug_gen)  \n",
    "    im = (im - np.min(im))/np.ptp(im) # to normalize all images --> matplotlib only takes pos. values between 0..1 / 0..255 \n",
    "    imgs = list(im)\n",
    "    labels = list(label)\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.suptitle('Augmented Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "    \n",
    "    i = 0 \n",
    "    for im, l  in zip(imgs, labels):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(im)\n",
    "        i = i+1\n",
    "        ax.set_title(\"Label: {}\".format(l))\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "        if i == batchsize:\n",
    "            break\n",
    "\n",
    "    #fig.tight_layout()\n",
    "    #fig.savefig('Augmented-Images.png', dpi=100)\n",
    "\n",
    "looking_at_augmentation(datagen, batchsize=9)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                      path  \\\n",
       "10621   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "53943   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "65143   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "122435  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "25075   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "93930   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "130915  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "3204    /home/Deep_Learner/work/local/histopathologic-...   \n",
       "130841  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "144100  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "77764   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "118893  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "29054   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "156158  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "35634   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "16715   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "35808   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "52627   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "121944  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "123986  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "14258   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "148741  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "18837   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "113119  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "68173   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "68572   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "37994   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "149062  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "128320  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "139945  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "...                                                   ...   \n",
       "24829   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "78372   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "68241   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "30278   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "15178   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "27717   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "55865   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "16958   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "494     /home/Deep_Learner/work/local/histopathologic-...   \n",
       "131433  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "9748    /home/Deep_Learner/work/local/histopathologic-...   \n",
       "40263   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "15670   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "68720   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "153227  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "69555   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "18509   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "135259  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "51192   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "118588  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "145481  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "130478  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "53344   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "73350   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "84444   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "124607  /home/Deep_Learner/work/local/histopathologic-...   \n",
       "57539   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "19451   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "14054   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "31682   /home/Deep_Learner/work/local/histopathologic-...   \n",
       "\n",
       "                                              id  label  \\\n",
       "10621   f34465b1428964166fe603e4e42560979d860a62      0   \n",
       "53943   00dfc8e9c08b00815431b7e43c37e3d463dfb477      0   \n",
       "65143   f921991c419c7099479a228ddc3bc0c93b8daef1      0   \n",
       "122435  faa5f3c87dd23af7d17e964ff8cc7fcc7309b901      1   \n",
       "25075   de5ec35d62abae628a577dacfbc9b064a4ca4cf9      0   \n",
       "93930   a53f4ec5a944da127f68ceb17eff13cb64c2d64f      1   \n",
       "130915  54354dccf78439ae05275cfc0b62c33fdeef60f1      1   \n",
       "3204    d8099496a9a31aa6b14fd7cad12aac67d3e0527f      0   \n",
       "130841  1e136b91a28c9bb8a247956855f630a2f421c75e      1   \n",
       "144100  f3008ef83d6cfa898c564aaeb55ac21dd77b6535      1   \n",
       "77764   a7db755d746482ff7c31903de67aaf199948b186      0   \n",
       "118893  4bccb478b24caa4488df07047098d12d0b1b939d      1   \n",
       "29054   cb79dc32ec56e706e71564de6c7ebb291c965067      0   \n",
       "156158  df1abfd0d0ba92d2cc7af440c54bd5b042345e99      1   \n",
       "35634   0e28162db026017b8fb95dc6ad37fc95ea0ecefb      0   \n",
       "16715   386fb5d5b34ba6bb8c3ee8cecc12b527091f088f      0   \n",
       "35808   04507c204fe5f732640c5f939cbe612135cc14ae      0   \n",
       "52627   bf3cf37f7c6a276735397c4cb9aab1f73096de69      0   \n",
       "121944  b6237579a9044af5b9fe1620f3b77303ed637233      1   \n",
       "123986  ed072ff21e2001b0da46b5cac306624b2868c62e      1   \n",
       "14258   019a2902f7b5dd13b251828988165fe14ab40b23      0   \n",
       "148741  1c2b599b12c9af8713316545f23e1bb29b691b3f      1   \n",
       "18837   d276b3f8149922a04a0d9f07b80627d0397d51f4      0   \n",
       "113119  f195663e6bf7cc09820996b9da22cc973a16c6cd      1   \n",
       "68173   a52b6a0a5904f9e3d7cb28cebee5cc2d396c9424      0   \n",
       "68572   1019c835ff4462c67f0dd9190096ac798736f9a1      0   \n",
       "37994   f5dccd2aefe78430b60f7f224fe98a81325bd9a5      0   \n",
       "149062  60d9250d8d04c3bb8d6c33bdb2c05a3283fb45d7      1   \n",
       "128320  26ebfdd9761f5262342f2f42074b5896fc79642c      1   \n",
       "139945  0f886f06072898caa0373410f815970c221a905d      1   \n",
       "...                                          ...    ...   \n",
       "24829   948e7e1c3daf88671600a7704bedde19546b9d80      0   \n",
       "78372   44a59963a54bac5ae37a3f999a20c4458f34e7be      0   \n",
       "68241   9bcab99fd1b989a9fc48dcdab9d3556c51b6db68      0   \n",
       "30278   32a89c4953d91006d2796aa64d9cbf23631449a1      0   \n",
       "15178   de3a9f775acc80ea0e8cf2669339a31dd0f1166a      0   \n",
       "27717   bd8b6225e966effe1a0bd4edb55a9386d47294f6      0   \n",
       "55865   4b15801906c6ccf1d5d507642b55c8fa43effaeb      0   \n",
       "16958   ecddb23e55480ab8c8c5a3608c0d19742793e27f      0   \n",
       "494     c799438069113d1a6d568cc8bdb3bbc15eceb20b      0   \n",
       "131433  82785462921690c64deb72bd81769a471faf80ec      1   \n",
       "9748    a855b3511feead7de5349ebfd3b6d7969330f5b3      0   \n",
       "40263   051010fe6689ce4a9afa4790cd527ee18bd0ac83      0   \n",
       "15670   e2ac50dc9aa1e5982b4920bf9eb2d9d34b62a8e9      0   \n",
       "68720   b05a353d6d614423514b22d12fee3a738c56d458      0   \n",
       "153227  e6e7c41746f0a15d6c2efc0d515503379a3df4ab      1   \n",
       "69555   73607db6658a118ccc4d0eb6b99ab3416f28be4b      0   \n",
       "18509   d0dd112eb675dee43e6854660afeb7202f5abc6b      0   \n",
       "135259  25c00e9fc1efa021fd9ed07b4ad03c5ce4906f3f      1   \n",
       "51192   c1f5a71dc0bb206eff7947b90b7038fcd9a6452b      0   \n",
       "118588  f35e95b0717a58ee8f9ac1896bba52c632d26fcc      1   \n",
       "145481  fda09c09390549243129bf659185b1f8c5d78ebc      1   \n",
       "130478  15ee8f8148f98f1a48afdbc8c0312ddb43b605da      1   \n",
       "53344   e84f52fd2a48b5ac414edccfafa38173d0b6826a      0   \n",
       "73350   46cd7c6ad33596d9406db12cdea86590bb7c6ef6      0   \n",
       "84444   8614e5959671985c29e775a95013597f99146bbe      1   \n",
       "124607  2ec05f7d86ec6a1d0f3dd9a2750d0593f5eacdb2      1   \n",
       "57539   816d596963fffec16e722ca6033aec57855f12d9      0   \n",
       "19451   e7692d20fbbd3227c58f05f0165c7974adc4668c      0   \n",
       "14054   a17937c16f261fd94d7def7118754ae87f17a6e7      0   \n",
       "31682   158b584bffb546ac3aae248bf3a4e8ac2b897a4a      0   \n",
       "\n",
       "                                                    image  \n",
       "10621   [[[103, 63, 98], [84, 44, 81], [79, 39, 76], [...  \n",
       "53943   [[[204, 176, 191], [245, 219, 232], [255, 241,...  \n",
       "65143   [[[245, 204, 236], [255, 223, 246], [249, 234,...  \n",
       "122435  [[[214, 146, 187], [189, 123, 161], [173, 113,...  \n",
       "25075   [[[255, 244, 252], [247, 230, 238], [255, 242,...  \n",
       "93930   [[[126, 58, 105], [110, 43, 86], [161, 95, 133...  \n",
       "130915  [[[136, 79, 186], [65, 9, 118], [86, 34, 142],...  \n",
       "3204    [[[158, 123, 179], [212, 180, 229], [237, 205,...  \n",
       "130841  [[[147, 87, 123], [140, 80, 118], [80, 22, 63]...  \n",
       "144100  [[[180, 146, 170], [196, 164, 187], [255, 245,...  \n",
       "77764   [[[171, 105, 143], [131, 60, 102], [156, 83, 1...  \n",
       "118893  [[[152, 74, 132], [145, 73, 136], [166, 101, 1...  \n",
       "29054   [[[244, 240, 237], [245, 237, 235], [253, 243,...  \n",
       "156158  [[[138, 93, 134], [152, 118, 151], [208, 189, ...  \n",
       "35634   [[[209, 148, 179], [204, 135, 166], [248, 172,...  \n",
       "16715   [[[212, 169, 196], [212, 165, 197], [215, 158,...  \n",
       "35808   [[[224, 183, 255], [85, 46, 137], [40, 12, 97]...  \n",
       "52627   [[[212, 157, 180], [225, 170, 193], [241, 186,...  \n",
       "121944  [[[255, 210, 225], [216, 161, 182], [244, 184,...  \n",
       "123986  [[[194, 160, 193], [223, 192, 223], [255, 241,...  \n",
       "14258   [[[196, 169, 184], [225, 204, 213], [230, 215,...  \n",
       "148741  [[[223, 160, 203], [179, 118, 160], [149, 92, ...  \n",
       "18837   [[[239, 232, 239], [239, 232, 239], [239, 232,...  \n",
       "113119  [[[135, 72, 163], [170, 101, 192], [153, 74, 1...  \n",
       "68173   [[[247, 206, 220], [242, 197, 217], [210, 157,...  \n",
       "68572   [[[94, 48, 84], [154, 110, 146], [93, 51, 87],...  \n",
       "37994   [[[225, 223, 224], [221, 221, 221], [225, 225,...  \n",
       "149062  [[[166, 106, 140], [169, 109, 145], [143, 81, ...  \n",
       "128320  [[[205, 150, 218], [176, 123, 195], [126, 75, ...  \n",
       "139945  [[[132, 73, 163], [149, 90, 184], [111, 53, 15...  \n",
       "...                                                   ...  \n",
       "24829   [[[224, 217, 224], [224, 217, 224], [223, 216,...  \n",
       "78372   [[[41, 22, 67], [57, 34, 78], [46, 15, 57], [3...  \n",
       "68241   [[[91, 67, 165], [82, 57, 151], [36, 7, 99], [...  \n",
       "30278   [[[227, 172, 191], [230, 172, 194], [189, 131,...  \n",
       "15178   [[[232, 240, 225], [255, 253, 255], [255, 234,...  \n",
       "27717   [[[142, 87, 191], [163, 107, 208], [92, 36, 12...  \n",
       "55865   [[[155, 109, 173], [117, 72, 129], [102, 59, 1...  \n",
       "16958   [[[124, 63, 104], [130, 69, 110], [137, 75, 11...  \n",
       "494     [[[149, 107, 129], [195, 156, 175], [202, 166,...  \n",
       "131433  [[[238, 205, 216], [231, 195, 209], [225, 184,...  \n",
       "9748    [[[247, 203, 236], [110, 65, 104], [63, 17, 64...  \n",
       "40263   [[[238, 168, 192], [234, 169, 191], [222, 163,...  \n",
       "15670   [[[36, 24, 96], [39, 36, 115], [105, 90, 183],...  \n",
       "68720   [[[242, 236, 238], [247, 243, 244], [248, 248,...  \n",
       "153227  [[[255, 251, 255], [201, 187, 223], [156, 128,...  \n",
       "69555   [[[166, 151, 156], [150, 134, 145], [61, 44, 6...  \n",
       "18509   [[[84, 42, 144], [39, 7, 104], [114, 91, 181],...  \n",
       "135259  [[[68, 31, 82], [158, 120, 167], [195, 153, 19...  \n",
       "51192   [[[96, 55, 87], [141, 98, 128], [232, 187, 218...  \n",
       "118588  [[[113, 46, 139], [87, 21, 111], [135, 71, 158...  \n",
       "145481  [[[138, 94, 127], [161, 116, 147], [196, 152, ...  \n",
       "130478  [[[249, 180, 211], [225, 156, 187], [211, 144,...  \n",
       "53344   [[[207, 189, 203], [89, 66, 86], [41, 6, 36], ...  \n",
       "73350   [[[74, 41, 94], [107, 69, 110], [248, 201, 235...  \n",
       "84444   [[[155, 106, 172], [167, 117, 186], [216, 164,...  \n",
       "124607  [[[166, 127, 180], [223, 165, 224], [188, 119,...  \n",
       "57539   [[[197, 176, 207], [105, 82, 111], [54, 27, 58...  \n",
       "19451   [[[248, 230, 244], [240, 222, 238], [219, 200,...  \n",
       "14054   [[[165, 88, 182], [158, 69, 163], [185, 78, 17...  \n",
       "31682   [[[241, 240, 236], [246, 245, 241], [248, 247,...  \n",
       "\n",
       "[160000 rows x 4 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_SIZE = 96\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "SAMPLE_SIZE = 80000\n",
    "file_path = ('/home/Deep_Learner/work/local/histopathologic-cancer-detection/')\n",
    "\n",
    "# Setup the data_frame: Path, ID, Label, Image\n",
    "training_dir = file_path + 'train/'\n",
    "data_frame = pd.DataFrame({'path': glob(os.path.join(training_dir,'*.tif'))})#getting the path into df\n",
    "data_frame['id'] = data_frame.path.map(lambda x: x.split('/')[7].split('.')[0]) #splitting the id-name(the 8th / in the path name) out of path\n",
    "\n",
    "labels = pd.read_csv(file_path + 'train_labels.csv')#reading labels\n",
    "\n",
    "data_frame = data_frame.merge(labels, on = 'id')#merging labels with df\n",
    "negatives = data_frame[data_frame.label == 0].sample(SAMPLE_SIZE)\n",
    "positives = data_frame[data_frame.label == 1].sample(SAMPLE_SIZE)\n",
    "\n",
    "data_frame = pd.concat([negatives, positives]).reset_index()\n",
    "\n",
    "data_frame = data_frame[['path', 'id', 'label']]\n",
    "data_frame['image'] = data_frame['path'].map(imread)\n",
    "\n",
    "data_frame = shuffle(data_frame)\n",
    "data_frame.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = 4\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=101, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for stratified-k-fold\n",
    "imgs = []\n",
    "for img in data_frame['image']:\n",
    "    imgs.append(img)\n",
    "        \n",
    "X = np.asarray(imgs)\n",
    "y = np.asarray(data_frame['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 96, 96, 3)\n",
      "[0 0 0 ... 0 0 0]\n",
      "(160000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataaugmentation\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "sometimes = lambda aug: iaa.Sometimes(0.4, aug)\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        # apply the following augmenters to most images\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "        iaa.Flipud(0.5), # vertically flip 20% of all images\n",
    "        sometimes(iaa.Affine(\n",
    "            scale={\"x\": (0.9, 1.2), \"y\": (0.9, 1.2)}, #>0.9 will cut part of img in combination with translate\n",
    "            translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)}, # >20% will also cut part of img\n",
    "            rotate=(-10, 10), # 45/-45° -> works good with scale + translate to prevent cuts\n",
    "            #shear=(-5, 5), # shear by -16 to +16 degrees\n",
    "            mode=ia.ALL \n",
    "        )),\n",
    "        iaa.SomeOf((0, 3), [\n",
    "                sometimes(iaa.Superpixels(p_replace=(0.2, 0.5), n_segments=(10, 100))), #superpixel-representation --> better basallamina representation \n",
    "                iaa.OneOf([\n",
    "                    iaa.GaussianBlur((0, 0.1)), #small blur effects --> better representation\n",
    "                    #iaa.AverageBlur(k=(1, 3)), # k must be odd\n",
    "                    #iaa.MedianBlur(k=(1, 3)), # \n",
    "                ]),\n",
    "                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)), #cell wall represenation\n",
    "                iaa.Emboss(alpha=(0, 0.8), strength=(0, 0.5)), #cell wall represenation\n",
    "                #searching for edges or angles --> blobby mask --> better basallamina representation / nuclei\n",
    "                iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                    iaa.EdgeDetect(alpha=(0.2, 0.4)), #detects edges --> cell wall,..\n",
    "                    iaa.DirectedEdgeDetect(alpha=(0.2, 0.4), direction=(0.0, 1.0)), #direction will make edges from random directions \n",
    "                ])),\n",
    "                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.2), # add gaussian noise to images\n",
    "             iaa.OneOf([\n",
    "                    iaa.Dropout((0.01, 0.03), per_channel=0.2), #rnd remove 5% in small pixels\n",
    "                    iaa.CoarseDropout((0.01, 0.03), size_percent=(0.01, 0.02), per_channel=0.2),# rnd remove 3% in big pixels\n",
    "                ]),\n",
    "                #iaa.Invert(0.01, per_channel=True), # invert color channels\n",
    "                #iaa.Add((-10, 10), per_channel=0.3), # change brightness of images (by -10 to 10 of original value)\n",
    "                iaa.AddToHueAndSaturation((-0.1, 0.1)), # change hue and saturation\n",
    "                #\n",
    "                #either change the brightness of the whole image (sometimes per channel) or change the brightness of subareas\n",
    "                iaa.OneOf([\n",
    "                    #iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
    "                    #iaa.FrequencyNoiseAlpha(\n",
    "                        #exponent=(-1, 0),\n",
    "                        #first=iaa.Multiply((0.9, 1.1), per_channel=True),\n",
    "                        #second=iaa.ContrastNormalization((0.9, 1.1))\n",
    "                    #)\n",
    "                ]),\n",
    "                sometimes(iaa.ElasticTransformation(alpha=(0, 0.5), sigma=0.1)), #still not sure: move pixels locally around\n",
    "                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.02))), #still not sure:move parts of the image around\n",
    "                #sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
    "            ],\n",
    "                     random_order=True\n",
    "        )\n",
    "    ],\n",
    "    random_order=True\n",
    ")\n",
    "\n",
    "datagen = ImageDataGenerator(preprocessing_function=seq.augment_image)\n",
    "datagen_test = ImageDataGenerator(horizontal_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "train_batch_size_2 = 256\n",
    "val_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 39910  39916  39917 ... 159997 159998 159999] TEST: [    0     1     2 ... 40075 40078 40079]\n",
      "[Fold 1/4]\n",
      "Model loaded.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 96, 96, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 96, 96, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 96, 96, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 48, 48, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 48, 48, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 24, 24, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 24, 24, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 24, 24, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 14,846,273\n",
      "Trainable params: 131,585\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 834s 445ms/step - loss: 1.4434 - acc: 0.7397 - val_loss: 0.3909 - val_acc: 0.8257\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.82570, saving model to VGG_fold_1_weights.h5\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a2a5ec1c14bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m#get data-generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch_size_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mval_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mmodel_VGG_bt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_classif_VGG_base_trainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deep_learning_python3/lib/python3.6/site-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mflow\u001b[0;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0msave_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             subset=subset)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     def flow_from_directory(self, directory,\n",
      "\u001b[0;32m/opt/conda/envs/deep_learning_python3/lib/python3.6/site-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[1;32m   1626\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_misc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_misc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/deep_learning_python3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#doesnt match the unshuffled numbers???\n",
    "i=0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    i=i+1\n",
    "    #slice training-data into folds for training/testing variables\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    #get .fit values\n",
    "    num_train_samples = len(X_train)\n",
    "    num_val_samples = len(X_valid)  \n",
    "    train_steps = np.ceil(num_train_samples // train_batch_size)\n",
    "    val_steps = np.ceil(num_val_samples // val_batch_size)\n",
    "    \n",
    "    #get data-generators\n",
    "    train_generator = datagen.flow(X_train, y_train, batch_size = train_batch_size)\n",
    "    val_generator = datagen_test.flow(X_valid, y_valid, batch_size = val_batch_size)\n",
    "    \n",
    "    #Callbacks & Model\n",
    "    name_weights = (base_path + \"\\Fold_\" + str(i) + \"{}.h5\" .format(model_name))\n",
    "    callbacks_list = get_callbacks(name_weights = name_weights)\n",
    "    model_VGG = get_model_classif_VGG_base_nottrainable()\n",
    " \n",
    "    #first on small batch_size to establish the basic informations\n",
    "    history = model_VGG.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=5, verbose=1,\n",
    "        validation_data = val_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks_list)\n",
    "\n",
    "    #second round --> whole model with larger batchsize\n",
    "    num_train_samples = len(X_train)\n",
    "    num_val_samples = len(X_valid)  \n",
    "    train_steps = np.ceil(num_train_samples // train_batch_size_2)\n",
    "    val_steps = np.ceil(num_val_samples // val_batch_size)\n",
    "    \n",
    "    #get data-generators\n",
    "    train_generator = datagen.flow(X_train, y_train, batch_size = train_batch_size_2)\n",
    "    val_generator = datagen_test.flow(X_valid, y_valid, batch_size = val_batch_size) \n",
    "    model_VGG_bt = get_model_classif_VGG_base_trainable()\n",
    "    model_VGG_bt.load_weights(name_weights)\n",
    "        \n",
    "    history = model_VGG_bt.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=50, verbose=1,\n",
    "        validation_data = val_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks_list)\n",
    "    \n",
    "    print(model_VGG.evaluate(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = file_path + '/test'\n",
    "df_test = pd.DataFrame({'path': glob(os.path.join(test_dir,'*.tif'))})\n",
    "df_test['id'] = df_test.path.map(lambda x: x.split('/')[8].split('.')[0])\n",
    "df_test['image'] = df_test['path'].map(imread)\n",
    "\n",
    "#make flow_from_dataframe work\n",
    "datagen_test = ImageDataGenerator(horizontal_flip=False)\n",
    "test_gen = datagen_test.flow_from_dataframe(df_test, \n",
    "                                            directory=test_dir, \n",
    "                                            x_col='id', \n",
    "                                            y_col=None, \n",
    "                                            has_ext=False, \n",
    "                                            batch_size = 1, \n",
    "                                            shuffle=False, \n",
    "                                            class_mode = None, \n",
    "                                            target_size = (IMAGE_SIZE,IMAGE_SIZE))\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way of loading testdata\n",
    "test_files = glob('/home/Deep_Learner/work/local/histopathologic-cancer-detection/test/*.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate 4 different models\n",
    "model_NASNet_fold1 = get_model_classif_nasnetm()\n",
    "model_NASNet_fold2 = get_model_classif_nasnetm()\n",
    "model_NASNet_fold3 = get_model_classif_nasnetm()\n",
    "model_NASNet_fold4 = get_model_classif_nasnetm()\n",
    "\n",
    "#Loading different k_fold-states\n",
    "model_NASNet_fold1.load_weights('Final_NASNetMobile_fold_1_weights.h5')\n",
    "print('Model model_NASNet_fold1 loaded!')\n",
    "model_NASNet_fold2.load_weights('Final_NASNetMobile_fold_2_weights.h5')\n",
    "print('Model model_NASNet_fold2 loaded!')\n",
    "model_NASNet_fold3.load_weights('Final_NASNetMobile_fold_3_weights.h5')\n",
    "print('Model model_NASNet_fold3 loaded!')\n",
    "model_NASNet_fold4.load_weights('Final_NASNetMobile_fold_4_weights.h5')\n",
    "print('Model model_NASNet_fold4 loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#short Validation to see if its working\n",
    "print(model_NASNetM.evaluate(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_file_path(file_path):\n",
    "    return file_path.split(os.path.sep)[-1].replace('.tif', '')\n",
    "\n",
    "#dataaugmentation from https://imgaug.readthedocs.io/en/latest/index.html\n",
    "def chunker(seq, size): #slices input(seq) into batches with the size=batch_size\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "preds = []\n",
    "ids = []\n",
    "for batch in chunker(test_files, batch_size):\n",
    "    X = [preprocess_input(cv2.imread(x).astype(np.float32)) for x in batch]\n",
    "    ids_batch = [get_id_from_file_path(x) for x in batch]\n",
    "    X = np.array(X)\n",
    "    preds_batch = ((model_NASNet_fold1.predict(X).ravel()*model_NASNet_fold2.predict(X[:, ::-1, :, :]).ravel()*model_NASNet_fold3.predict(X[:, ::-1, ::-1, :]).ravel()*model_NASNet_fold4.predict(X[:, :, ::-1, :]).ravel())**0.25).tolist()\n",
    "    preds += preds_batch\n",
    "    ids += ids_batch\n",
    "    print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':ids, 'label':preds})\n",
    "df.to_csv(\"baseline0_k_fold_crossvalidation.csv\", index=False)\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c histopathologic-cancer-detection -f baseline0_k_fold_crossvalidation.csv -m \"k-fold-crossvalidation NASNetM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the data_frame: Path, ID, Label, Image\n",
    "crossval_path = r\"C:\\Users\\eg38emed\\FCD\\Tiles\\Train_crossvalidation\"\n",
    "\n",
    "fcd = crossval_path + '\\FCD'\n",
    "tsc = crossval_path + '\\TSC'\n",
    "\n",
    "#fcd folder\n",
    "df = pd.DataFrame({'path': glob(os.path.join(fcd,'*.png'))})\n",
    "df['id'] = df.path.map(lambda x: x.split('\\\\')[6].split('.')[0]) \n",
    "df['label'] = 0\n",
    "\n",
    "#tsc folder\n",
    "df_pos = pd.DataFrame({'path': glob(os.path.join(tsc,'*.png'))})\n",
    "df_pos['id'] = df_pos.path.map(lambda x: x.split('\\\\')[6].split('.')[0]) \n",
    "df_pos['label'] = 1\n",
    "\n",
    "#both\n",
    "df_train = pd.concat([df, df_pos])\n",
    "\n",
    "#add images\n",
    "df_train['image'] = df_train['path'].map(imread)\n",
    "\n",
    "print(len(df_train))\n",
    "print(df_train)\n",
    "\n",
    "#initialize StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = 5\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=101, shuffle = False)\n",
    "\n",
    "train_batch_size = 64\n",
    "val_batch_size = 64\n",
    "\n",
    "#datagenerators\n",
    "datagen_train = ImageDataGenerator(preprocessing_function=seq.augment_image,\n",
    "                            rescale=1./255)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#doesnt match the unshuffled numbers???\n",
    "i=0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    i=i+1\n",
    "    #slice training-data into folds for training/testing variables\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    #get .fit values\n",
    "    num_train_samples = len(X_train)\n",
    "    num_val_samples = len(X_valid)  \n",
    "    train_steps = np.ceil(num_train_samples // train_batch_size)\n",
    "    val_steps = np.ceil(num_val_samples // val_batch_size)\n",
    "    \n",
    "    #get data-generators\n",
    "    train_generator = datagen_train.flow(X_train, y_train, batch_size = train_batch_size)\n",
    "    val_generator = datagen_val.flow(X_valid, y_valid, batch_size = val_batch_size)\n",
    "    \n",
    "    #Callbacks & Model\n",
    "    name_weights = (base_path + \"\\Fold_\" + str(i) + \"{}.h5\" .format(model_name))\n",
    "    callbacks_list = get_callbacks(name_weights = name_weights)\n",
    "    model_VGG = get_model_classif_VGG_base_nottrainable()\n",
    " \n",
    "    #first on small batch_size to establish the basic informations\n",
    "    history = model_VGG.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=5, verbose=1,\n",
    "        validation_data = val_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks_list)\n",
    "\n",
    "    #second round -> whole model with larger batchsize\n",
    "    num_train_samples = len(X_train)\n",
    "    num_val_samples = len(X_valid)  \n",
    "    train_steps = np.ceil(num_train_samples // train_batch_size)\n",
    "    val_steps = np.ceil(num_val_samples // val_batch_size)\n",
    "    \n",
    "    #get data-generators\n",
    "    train_generator = datagen_train.flow(X_train, y_train, batch_size = train_batch_size)\n",
    "    val_generator = datagen_val.flow(X_valid, y_valid, batch_size = val_batch_size) \n",
    "    model_VGG_bt = get_model_classif_VGG_base_trainable()\n",
    "    model_VGG_bt.load_weights(name_weights)\n",
    "        \n",
    "    history = model_VGG_bt.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=50, verbose=1,\n",
    "        validation_data = val_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks_list)\n",
    "    \n",
    "    print(model_VGG.evaluate(X_valid, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
