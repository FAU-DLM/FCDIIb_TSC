{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASNETLarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#visualize Augmentation from directory!\n",
    "def looking_at_augmentation (data_generator, batchsize):\n",
    "    im, label = next(data_generator)  \n",
    "    im = (im - np.min(im))/np.ptp(im) # to normalize all images --> matplotlib only takes pos. values between 0..1 / 0..255 \n",
    "    imgs = list(im)\n",
    "    labels = list(label)\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.suptitle('Augmented Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, im  in enumerate(imgs[:batchsize]):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(im)\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "    #fig.tight_layout()\n",
    "    fig.savefig('Augmented-Images.png', dpi=300)\n",
    "\n",
    "#fast plot of training history\n",
    "def plot_history(history, modelname, path):\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n",
    "    axs[0].plot(hist_df.val_categorical_accuracy, lw=5, label='Validation Accuracy')\n",
    "    axs[0].plot(hist_df.categorical_accuracy, lw=5, label='Training Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].grid()\n",
    "    axs[0].legend(loc=0)\n",
    "    axs[1].plot(hist_df.val_loss, lw=5, label='Validation MLogLoss')\n",
    "    axs[1].plot(hist_df.loss, lw=5, label='Training MLogLoss')\n",
    "    axs[1].set_ylabel('MLogLoss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].grid()\n",
    "    axs[1].legend(loc=0)\n",
    "    fig.savefig(path + '\\History_{}.png' .format(modelname), dpi=300)\n",
    "    plt.show();\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc#plotting the receiver operating characteristics --> evaluate performance cutting point vice\n",
    "def plot_roc(label, predictions, modelname, path): #IDEA: set diffrent cutting point based on ROC for ensembling   \n",
    "    roc_auc_score(label, predictions)\n",
    "    print('The ROC-Score is: {}' .format(roc_auc_score))\n",
    "\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(label, predictions)\n",
    "    auc_keras = auc(fpr_keras, tpr_keras)\n",
    "    #print(auc_keras)\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve: {}' .format(auc_keras))\n",
    "    plt.legend(loc='best')\n",
    "    fig.savefig(path + '\\ROC-Curve_{}.png' .format(modelname), dpi=300) #saving PLOT \n",
    "    plt.show()\n",
    "    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "# Source: Scikit Learn website\n",
    "# http://scikit-learn.org/stable/auto_examples/\n",
    "# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n",
    "# selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + '\\Confusion-Matrix.png', dpi=300)\n",
    "\n",
    "#plotting correctly classified images: https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python\n",
    "def plot_correct(vals, y_pred, y_label, modelname, path):\n",
    "    correct = np.where(y_pred==y_label)[0]\n",
    "    print (\"Found %d correct labels\" % len(correct))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.suptitle('Correct Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, correct in enumerate(correct[:9]):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(vals[correct])\n",
    "        ax.set_title(\"Predicted {}, Class {}\".format(y_pred[correct], y_label[correct]), fontsize=10)\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "    fig.savefig(path + '\\Correct_Images_{}.png' .format(modelname), dpi=100) #saving PLOT \n",
    "\n",
    "#Plotting incorrectly classified\n",
    "def plot_incorrect(vals, y_pred, y_label, modelname, path):\n",
    "    incorrect = np.where(y_pred!=y_label)[0]\n",
    "    print (\"Found %d incorrect labels\" % len(incorrect))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.suptitle('Incorrect Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, incorrect in enumerate(incorrect[:9]):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(vals[incorrect])\n",
    "        ax.set_title(\"Predicted {}, Class {}\".format(y_pred[incorrect], y_label[incorrect]), fontsize=10)\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "    fig.savefig(path + '\\Incorrect_Images_{}.png' .format(modelname), dpi=100) #saving PLOT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FCD', 'Train', 'TSC', 'Validation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "print(os.listdir(r\"D:\\Doktorarbeit\\FCD\\Tiles\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "from time import time\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution1D, concatenate, SpatialDropout1D, GlobalMaxPool1D, GlobalAvgPool1D, Embedding, \\\n",
    "    Conv2D, SeparableConv1D, Add, BatchNormalization, Activation, GlobalAveragePooling2D, LeakyReLU, Flatten\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, \\\n",
    "    Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n",
    "from keras.layers.pooling import _GlobalPooling1D\n",
    "from keras.losses import mae, sparse_categorical_crossentropy, binary_crossentropy\n",
    "from keras.models import Model\n",
    "from keras.applications.nasnet import NASNetMobile, NASNetLarge, preprocess_input\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint, Callback, LambdaCallback\n",
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        # apply the following augmenters to most images\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "        iaa.Flipud(0.5), # vertically flip 20% of all images\n",
    "        sometimes(iaa.Affine(\n",
    "            scale={\"x\": (0.9, 1.6), \"y\": (0.9, 1.6)}, #>20 will cut part of img\n",
    "            translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)}, # >20% will also cut part of img\n",
    "            rotate=(-10, 10), # 45/-45° -> works good with scale + translate to prevent cuts\n",
    "            shear=(-5, 5), # shear by -16 to +16 degrees\n",
    "            mode=ia.ALL \n",
    "        )),\n",
    "        iaa.SomeOf((0, 4), [\n",
    "                sometimes(iaa.Superpixels(p_replace=(0.3, 0.7), n_segments=(10, 100))), #superpixel-representation --> better basallamina representation \n",
    "                iaa.OneOf([\n",
    "                    iaa.GaussianBlur((0, 0.1)), #small blur effects --> better representation\n",
    "                    #iaa.AverageBlur(k=(1, 3)), # k must be odd\n",
    "                    #iaa.MedianBlur(k=(1, 3)), # \n",
    "                ]),\n",
    "                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)), #cell wall represenation\n",
    "                iaa.Emboss(alpha=(0, 0.8), strength=(0, 0.5)), #cell wall represenation\n",
    "                #searching for edges or angles --> blobby mask --> better basallamina representation / nuclei\n",
    "                iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                    iaa.EdgeDetect(alpha=(0.2, 0.4)), #detects edges --> cell wall,..\n",
    "                    iaa.DirectedEdgeDetect(alpha=(0.2, 0.4), direction=(0.0, 1.0)), #direction will make edges from random directions \n",
    "                ])),\n",
    "                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.2), # add gaussian noise to images\n",
    "             iaa.OneOf([\n",
    "                    iaa.Dropout((0.05, 0.2), per_channel=0.2), #rnd remove 5% in small pixels\n",
    "                    iaa.CoarseDropout((0.05, 0.2), size_percent=(0.01, 0.02), per_channel=0.2),# rnd remove 3% in big pixels\n",
    "                ]),\n",
    "                iaa.Invert(0.01, per_channel=True), # invert color channels\n",
    "                iaa.Add((-10, 10), per_channel=0.3), # change brightness of images (by -10 to 10 of original value)\n",
    "                #iaa.AddToHueAndSaturation((-0.1, 0.1)), # change hue and saturation\n",
    "                #\n",
    "                #either change the brightness of the whole image (sometimes per channel) or change the brightness of subareas\n",
    "                iaa.OneOf([\n",
    "                    iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
    "                    iaa.FrequencyNoiseAlpha(\n",
    "                        exponent=(-1, 0),\n",
    "                        first=iaa.Multiply((0.9, 1.1), per_channel=True),\n",
    "                        second=iaa.ContrastNormalization((0.9, 1.1))\n",
    "                    )\n",
    "                ]),\n",
    "                sometimes(iaa.ElasticTransformation(alpha=(0, 0.5), sigma=0.1)), #still not sure: move pixels locally around\n",
    "                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.02))), #still not sure:move parts of the image around\n",
    "                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
    "            ],\n",
    "                     random_order=True\n",
    "        )\n",
    "    ],\n",
    "    random_order=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80647\n",
      "4086\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 272\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "model_name = 'NASNetM'\n",
    "\n",
    "base_path = (r'C:\\Users\\eg38emed\\FCD\\Models\\{}'.format(model_name))\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "    \n",
    "\n",
    "train_path = (r'C:\\Users\\eg38emed\\FCD\\Tiles\\Train')\n",
    "val_path = (r'C:\\Users\\eg38emed\\FCD\\Tiles\\Validation')\n",
    "\n",
    "num_fcd = (os.listdir(r\"C:\\Users\\eg38emed\\FCD\\Tiles\\Train\\FCD\"))\n",
    "num_tsc = (os.listdir(r\"C:\\Users\\eg38emed\\FCD\\Tiles\\Train\\TSC\"))\n",
    "\n",
    "num_fcd_val = (os.listdir(r\"C:\\Users\\eg38emed\\FCD\\Tiles\\Validation\\FCD\"))\n",
    "num_tsc_val = (os.listdir(r\"C:\\Users\\eg38emed\\FCD\\Tiles\\Validation\\TSC\"))\n",
    "\n",
    "num_train_samples = (len(num_fcd) + len(num_tsc))\n",
    "num_val_samples = (len(num_fcd_val) + len(num_tsc_val))\n",
    "print(num_train_samples)\n",
    "print(num_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80647 images belonging to 2 classes.\n",
      "Found 4086 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batch_size_1 = 64\n",
    "val_batch_size = 32\n",
    "#datagenerators\n",
    "datagen_train = ImageDataGenerator(preprocessing_function=seq.augment_image,\n",
    "                            rescale=1./255)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "train_gen_1 = datagen_train.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=train_batch_size_1,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "val_gen = datagen_val.flow_from_directory(val_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "looking_at_augmentation(val_gen, batchsize=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_classif_nasnetl(IMAGE_SIZE):\n",
    "    inputs = Input((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    base_model_NASNet = NASNetLarge(weights=None, include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) \n",
    "    x = base_model_NASNet(inputs)\n",
    "    out1 = GlobalMaxPooling2D()(x)\n",
    "    out2 = GlobalAveragePooling2D()(x)\n",
    "    out3 = Flatten()(x)\n",
    "    out = Concatenate(axis=-1)([out1, out2, out3])\n",
    "    out = Dropout(0.5)(out)\n",
    "    out = Dense(2, activation=\"softmax\", name=\"3_\")(out)\n",
    "    model = Model(inputs, out)\n",
    "    model.compile(optimizer=Adam(0.0001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_callbacks(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=15, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    return [earlystopping, ReduceLR, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_weights = (base_path + \"\\{}.h5\".format(model_name))\n",
    "callbacks_list = get_callbacks(name_weights = name_weights)\n",
    "model = get_model_classif_nasnetl(IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_gen_1, \n",
    "                    steps_per_epoch=train_gen_1.samples // train_batch_size_1, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // val_batch_size,\n",
    "                    epochs=100, verbose=1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, modelname = model_name, path=base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the data_frame: Path, ID, Label, Image\n",
    "path = val_path\n",
    "fcd = path + '\\FCD'\n",
    "tsc = path + '\\TSC'\n",
    "\n",
    "#fcd folder\n",
    "df = pd.DataFrame({'path': glob(os.path.join(fcd,'*.png'))})\n",
    "df['id'] = df.path.map(lambda x: x.split('\\\\')[6].split('.')[0]) \n",
    "df['label'] = 0\n",
    "\n",
    "#tsc folder\n",
    "df_pos = pd.DataFrame({'path': glob(os.path.join(tsc,'*.png'))})\n",
    "df_pos['id'] = df_pos.path.map(lambda x: x.split('\\\\')[6].split('.')[0]) \n",
    "df_pos['label'] = 1\n",
    "\n",
    "#both\n",
    "df_train = pd.concat([df, df_pos])\n",
    "\n",
    "#add images\n",
    "df_train['image'] = df_train['path'].map(imread)\n",
    "\n",
    "print(df_train.head())\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = datagen_test.flow_from_directory(val_path, \n",
    "                                                  batch_size = batch_size, \n",
    "                                                  shuffle=False,#FALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSEFALSE\n",
    "                                                  target_size=(96, 96), \n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting images out of df\n",
    "imgs = []\n",
    "for img in df_train['image']:\n",
    "    imgs.append(img)\n",
    "        \n",
    "X = np.asarray(imgs)\n",
    "y = to_categorical(df_train['label'])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=X[0]\n",
    "plt.imshow(im)\n",
    "print(df_train.iloc[0])\n",
    "filenames = test_generator.filenames\n",
    "print(filenames[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model on validation-data + test_datagen --> weights of best training epoch will be used\n",
    "model_VGG = get_model_classif_VGG_base_trainable()\n",
    "model_VGG.load_weights(\"VGG_FCD_All.h5\")\n",
    "val_loss, val_acc = \\\n",
    "model_VGG.evaluate_generator(test_generator, \n",
    "                             steps=len(df_train),\n",
    "                             verbose=1)\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "predictions = model_VGG.predict_generator(test_generator, \n",
    "                                          steps=len(df_train), \n",
    "                                          workers=6, \n",
    "                                          use_multiprocessing=False, \n",
    "                                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true labels\n",
    "y_label = np.argmax(y, axis=1)\n",
    "print(y_label)\n",
    "\n",
    "df_preds = pd.DataFrame(predictions, columns=['no_tumor_tissue', 'has_tumor_tissue'])\n",
    "print(df_preds.head())\n",
    "# Get the predicted labels as probabilities\n",
    "y_pred = df_preds['has_tumor_tissue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe change pos/neg cutoff:\n",
    "plot_roc(y_label, y_pred, modelname='VGG')\n",
    "\n",
    "cm = confusion_matrix(y_label, y_pred.round())\n",
    "cm_plot_labels = ['FCD', 'TSC']\n",
    "\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix VGG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rounding predictions + make them int --> compare to label / heatmap\n",
    "y_pred = y_pred.round()\n",
    "y_pred_int = []\n",
    "for i in y_pred:\n",
    "    if i == 1.0:\n",
    "        y_pred_int.append(1)\n",
    "    else:\n",
    "        y_pred_int.append(0)\n",
    "\n",
    "y_pred_int = np.asarray(y_pred_int)\n",
    "print((y_pred_int.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correct(X, y_pred_int, y_label, modelname='VGG')\n",
    "plot_incorrect(X, y_pred_int, y_label, modelname='VGG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASNetM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_classif_nasnetm():\n",
    "    inputs = Input((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    base_model_NASNet = NASNetMobile(include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    x = base_model_NASNet(inputs)\n",
    "    out1 = GlobalMaxPooling2D()(x)\n",
    "    out2 = GlobalAveragePooling2D()(x)\n",
    "    out3 = Flatten()(x)\n",
    "    out = Concatenate(axis=-1)([out1, out2, out3])\n",
    "    out = Dropout(0.5)(out)\n",
    "    out = Dense(2, activation=\"softmax\", name=\"3_\")(out)\n",
    "    model_NASNetM = Model(inputs, out)\n",
    "    model_NASNetM.compile(optimizer=Adam(0.0001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    model_NASNetM.summary()\n",
    "    return model_NASNetM\n",
    "\n",
    "def get_callbacks(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=15, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    return [earlystopping, ReduceLR, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 272, 272, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "NASNet (Model)                  (None, 9, 9, 1056)   4269716     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 1056)         0           NASNet[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 1056)         0           NASNet[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 85536)        0           NASNet[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 87648)        0           global_max_pooling2d_2[0][0]     \n",
      "                                                                 global_average_pooling2d_2[0][0] \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 87648)        0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "3_ (Dense)                      (None, 2)            175298      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,445,014\n",
      "Trainable params: 4,408,276\n",
      "Non-trainable params: 36,738\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "name_weights = (base_path + \"\\{}_272.h5\".format(model_name))\n",
    "callbacks_list = get_callbacks(name_weights = name_weights)\n",
    "model_NASNetM = get_model_classif_nasnetm()\n",
    "#model_NASNetM.load_weights(base_path + \"\\{}.h5\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1260/1260 [==============================] - 1466s 1s/step - loss: 0.4662 - acc: 0.7848 - val_loss: 1.0713 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69119, saving model to C:\\Users\\eg38emed\\FCD\\Models\\NASNetM\\NASNetM_272.h5\n",
      "Epoch 2/30\n",
      "1260/1260 [==============================] - 1412s 1s/step - loss: 0.2526 - acc: 0.8849 - val_loss: 0.8522 - val_acc: 0.7417\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.69119 to 0.74174, saving model to C:\\Users\\eg38emed\\FCD\\Models\\NASNetM\\NASNetM_272.h5\n",
      "Epoch 3/30\n",
      "1260/1260 [==============================] - 1412s 1s/step - loss: 0.1968 - acc: 0.9116 - val_loss: 0.7860 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.74174 to 0.75802, saving model to C:\\Users\\eg38emed\\FCD\\Models\\NASNetM\\NASNetM_272.h5\n",
      "Epoch 4/30\n",
      "1260/1260 [==============================] - 1412s 1s/step - loss: 0.1614 - acc: 0.9258 - val_loss: 0.8743 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.75802 to 0.76739, saving model to C:\\Users\\eg38emed\\FCD\\Models\\NASNetM\\NASNetM_272.h5\n",
      "Epoch 5/30\n",
      "1260/1260 [==============================] - 1407s 1s/step - loss: 0.1388 - acc: 0.9364 - val_loss: 0.4406 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.76739 to 0.85348, saving model to C:\\Users\\eg38emed\\FCD\\Models\\NASNetM\\NASNetM_272.h5\n",
      "Epoch 6/30\n",
      "1260/1260 [==============================] - 1407s 1s/step - loss: 0.1233 - acc: 0.9424 - val_loss: 0.6226 - val_acc: 0.7926\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.85348\n",
      "Epoch 7/30\n",
      "1260/1260 [==============================] - 1409s 1s/step - loss: 0.1142 - acc: 0.9478 - val_loss: 0.9784 - val_acc: 0.7536\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.85348\n",
      "Epoch 8/30\n",
      "1260/1260 [==============================] - 1412s 1s/step - loss: 0.1060 - acc: 0.9507 - val_loss: 1.5174 - val_acc: 0.7082\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.85348\n",
      "Epoch 9/30\n",
      "1260/1260 [==============================] - 1413s 1s/step - loss: 0.0971 - acc: 0.9540 - val_loss: 0.2843 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.85348 to 0.89739, saving model to C:\\Users\\eg38emed\\FCD\\Models\\NASNetM\\NASNetM_272.h5\n",
      "Epoch 10/30\n",
      "1260/1260 [==============================] - 1411s 1s/step - loss: 0.0931 - acc: 0.9559 - val_loss: 0.3996 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.89739\n",
      "Epoch 11/30\n",
      "1260/1260 [==============================] - 1410s 1s/step - loss: 0.0880 - acc: 0.9582 - val_loss: 0.3737 - val_acc: 0.8934\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89739\n",
      "Epoch 12/30\n",
      "1260/1260 [==============================] - 1409s 1s/step - loss: 0.0847 - acc: 0.9609 - val_loss: 0.5013 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.89739\n",
      "Epoch 13/30\n",
      "1260/1260 [==============================] - 1408s 1s/step - loss: 0.0797 - acc: 0.9622 - val_loss: 0.7550 - val_acc: 0.8379\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89739\n",
      "Epoch 14/30\n",
      "1260/1260 [==============================] - 1407s 1s/step - loss: 0.0771 - acc: 0.9626 - val_loss: 0.5285 - val_acc: 0.8537\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.89739\n",
      "Epoch 15/30\n",
      "1260/1260 [==============================] - 1407s 1s/step - loss: 0.0650 - acc: 0.9679 - val_loss: 0.7027 - val_acc: 0.8249\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89739\n",
      "Epoch 16/30\n",
      "1260/1260 [==============================] - 1409s 1s/step - loss: 0.0617 - acc: 0.9694 - val_loss: 0.5892 - val_acc: 0.8426\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89739\n",
      "Epoch 17/30\n",
      "1260/1260 [==============================] - 1407s 1s/step - loss: 0.0622 - acc: 0.9702 - val_loss: 0.3670 - val_acc: 0.8925\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89739\n",
      "Epoch 18/30\n",
      "1260/1260 [==============================] - 1406s 1s/step - loss: 0.0606 - acc: 0.9714 - val_loss: 0.4972 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89739\n",
      "Epoch 19/30\n",
      "1260/1260 [==============================] - 1408s 1s/step - loss: 0.0587 - acc: 0.9712 - val_loss: 0.3581 - val_acc: 0.9018\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.89739 to 0.90183, saving model to C:\\Users\\eg38emed\\FCD\\Models\\NASNetM\\NASNetM_272.h5\n",
      "Epoch 20/30\n",
      "1260/1260 [==============================] - 1405s 1s/step - loss: 0.0581 - acc: 0.9720 - val_loss: 0.5069 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.90183\n",
      "Epoch 21/30\n",
      "1260/1260 [==============================] - 1408s 1s/step - loss: 0.0567 - acc: 0.9735 - val_loss: 0.5337 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.90183\n",
      "Epoch 22/30\n",
      "1260/1260 [==============================] - 1411s 1s/step - loss: 0.0563 - acc: 0.9735 - val_loss: 0.4066 - val_acc: 0.8979\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.90183\n",
      "Epoch 23/30\n",
      "1260/1260 [==============================] - 1409s 1s/step - loss: 0.0581 - acc: 0.9732 - val_loss: 0.4075 - val_acc: 0.8853\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.90183\n",
      "Epoch 24/30\n",
      "  12/1260 [..............................] - ETA: 23:04 - loss: 0.0539 - acc: 0.9753"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-fcef35c340b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mval_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                     callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model_NASNetM.fit_generator(train_gen_1, \n",
    "                    steps_per_epoch=train_gen_1.samples // train_batch_size_1, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // val_batch_size,\n",
    "                    epochs=30, verbose=1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, modelname = model_name, path=base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications, optimizers\n",
    "def get_model_classif_X_base_nottrainable():\n",
    "    base_model = applications.Xception(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "    print('Model loaded.')\n",
    "    i=0\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False\n",
    "        print(i,layer.name)\n",
    "        i=i+1\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x) #from 0.2\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "    model_X = Model(inputs=base_model.input, outputs=predictions)\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "    model_X.compile(loss='binary_crossentropy', optimizer=adam, metrics = ['accuracy'])\n",
    "    model_X.summary()\n",
    "    \n",
    "    return model_X\n",
    "\n",
    "def get_model_classif_Xception():\n",
    "    base_model = applications.Xception(weights=None, include_top=False, input_shape=(96, 96, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x) #from 0.2\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "    model_X = Model(inputs=base_model.input, outputs=predictions)\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "    model_X.compile(loss='binary_crossentropy', optimizer=adam, metrics = ['accuracy'])\n",
    "    model_X.summary()\n",
    "\n",
    "    return model_X\n",
    "\n",
    "def get_callbacks(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=15, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    return [earlystopping, ReduceLR, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_weights = \"Xception_basenottrainable_FCD.h5\"\n",
    "callbacks_list = get_callbacks(name_weights = name_weights)\n",
    "model_X = get_model_classif_X_base_nottrainable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_X.fit_generator(train_gen_1, \n",
    "                    steps_per_epoch=train_gen_1.samples // train_batch_size_1, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // val_batch_size,\n",
    "                    epochs=20, verbose=1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history,modelname = 'Xception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_weights = \"Xception_FCD.h5\"\n",
    "callbacks_list = get_callbacks(name_weights = name_weights)\n",
    "model_X = get_model_classif_Xception()\n",
    "model_X.load_weights(\"Xception_basenottrainable_FCD.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_X.fit_generator(train_gen_1, \n",
    "                    steps_per_epoch=train_gen_1.samples // train_batch_size_1, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // val_batch_size,\n",
    "                    epochs=100, verbose=1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xception + NASNetM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_classif_nasnet_x():\n",
    "    # Model Idea Xception + NASNetM\n",
    "    input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    xception = applications.Xception(weights='imagenet', include_top=False, input_shape=input_shape)  \n",
    "    nas_net = NASNetMobile(weights=None, include_top=False, input_shape=input_shape)\n",
    "\n",
    "    outputs = Concatenate(axis=-1)([GlobalAveragePooling2D()(xception(inputs)),\n",
    "                                    GlobalAveragePooling2D()(nas_net(inputs))])\n",
    "    outputs = Dropout(0.5)(outputs)\n",
    "    outputs = Dense(2, activation='softmax')(outputs)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(lr=0.0001, decay=0.00001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_callbacks(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=15, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    return [earlystopping, ReduceLR, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_weights = \"NASNETM+Xception_FCD.h5\"\n",
    "callbacks_list = get_callbacks(name_weights = name_weights)\n",
    "model_NASNetM_X = get_model_classif_nasnet_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_NASNetM_X.fit_generator(train_gen_1, \n",
    "                    steps_per_epoch=train_gen_1.samples // train_batch_size_1, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // val_batch_size,\n",
    "                    epochs=100, verbose=1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history,modelname = 'NASNetM + Xception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
