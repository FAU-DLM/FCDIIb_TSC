{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold-Crossvalidation with a VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "import sys\n",
    "#sys.path.append(\"/home/Deep_Learner/work/network/keras_utils\") \n",
    "sys.path.append(r\"C:\\Users\\eg38emed\\FCD\\FCD vs TSC\\keras_utils\")\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras import backend\n",
    "from keras import applications\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D, Dense, Input, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, AveragePooling2D, Concatenate\n",
    "from image.image_data_generator import ImageDataGenerator\n",
    "from keras.preprocessing.image import array_to_img, img_to_array\n",
    "from image.directory_iterator import DirectoryIterator\n",
    "from image.dataframe_iterator import DataFrameIterator\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint, Callback, LambdaCallback, CSVLogger\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "\n",
    "from helper import *\n",
    "auc_roc = as_keras_metric(tf.metrics.auc)\n",
    "import clr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''path = '/home/Deep_Learner/work/local/Tiles_RCZ/FCD/'\n",
    "train_dir = '/home/Deep_Learner/work/local/Tiles_RCZ/Train/a_fcd/'\n",
    "file = '/home/Deep_Learner/work/local/FCD.csv'\n",
    "df=pd.read_csv(file)\n",
    "#print(df.column)\n",
    "df_fcd = df.drop('path', 1)\n",
    "#df_fcd = df_fcd.reset_index()\n",
    "print(fname)\n",
    "print(df_fcd.head())\n",
    "print(len(df_fcd))\n",
    "\n",
    "for j, image in enumerate(df_fcd['id']):\n",
    "        # get the label for a certain image\n",
    "        print(j)\n",
    "        print(image)\n",
    "        fname = image  \n",
    "        # source path to image\n",
    "        src = os.path.join(path, fname)\n",
    "        print(src)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(train_dir, fname)\n",
    "        print(dst)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(src, dst)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        # apply the following augmenters to most images\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "        iaa.Flipud(0.5), # vertically flip 20% of all images\n",
    "        sometimes(iaa.Affine(\n",
    "            scale={\"x\": (0.9, 1.6), \"y\": (0.9, 1.6)}, #>20 will cut part of img\n",
    "            translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)}, # >20% will also cut part of img\n",
    "            rotate=(-10, 10), # 45/-45Â° -> works good with scale + translate to prevent cuts\n",
    "            shear=(-5, 5), # shear by -16 to +16 degrees\n",
    "            mode=ia.ALL \n",
    "        )),\n",
    "        iaa.SomeOf((0, 4), [\n",
    "                sometimes(iaa.Superpixels(p_replace=(0.3, 0.7), n_segments=(10, 100))), #superpixel-representation --> better basallamina representation \n",
    "                iaa.OneOf([\n",
    "                    iaa.GaussianBlur((0, 0.2)), #small blur effects --> better representation\n",
    "                    iaa.AverageBlur(k=(1, 3)), # k must be odd\n",
    "                    iaa.MedianBlur(k=(1, 3)), # \n",
    "                ]),\n",
    "                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)), #cell wall represenation\n",
    "                iaa.Emboss(alpha=(0, 0.8), strength=(0, 0.5)), #cell wall represenation\n",
    "                #searching for edges or angles --> blobby mask --> better basallamina representation / nuclei\n",
    "                iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                    iaa.EdgeDetect(alpha=(0.2, 0.4)), #detects edges --> cell wall,..\n",
    "                    iaa.DirectedEdgeDetect(alpha=(0.2, 0.4), direction=(0.0, 1.0)), #direction will make edges from random directions \n",
    "                ])),\n",
    "                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.2), # add gaussian noise to images\n",
    "             iaa.OneOf([\n",
    "                    iaa.Dropout((0.05, 0.3), per_channel=0.2), #rnd remove 5% in small pixels\n",
    "                    iaa.CoarseDropout((0.05, 0.3), size_percent=(0.01, 0.02), per_channel=0.2),# rnd remove 3% in big pixels\n",
    "                ]),\n",
    "                iaa.Invert(0.01, per_channel=True), # invert color channels\n",
    "                iaa.Add((-10, 10), per_channel=0.3), # change brightness of images (by -10 to 10 of original value)\n",
    "                #iaa.AddToHueAndSaturation((-0.1, 0.1)), # change hue and saturation\n",
    "                #\n",
    "                #either change the brightness of the whole image (sometimes per channel) or change the brightness of subareas\n",
    "                iaa.OneOf([\n",
    "                    iaa.Multiply((0.9, 1.2), per_channel=0.5),\n",
    "                    iaa.FrequencyNoiseAlpha(\n",
    "                        exponent=(-1, 0),\n",
    "                        first=iaa.Multiply((0.9, 1.1), per_channel=True),\n",
    "                        second=iaa.ContrastNormalization((0.9, 1.1))\n",
    "                    )\n",
    "                ]),\n",
    "                sometimes(iaa.ElasticTransformation(alpha=(0, 0.5), sigma=0.1)), #still not sure: move pixels locally around\n",
    "                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.03))), #still not sure:move parts of the image around\n",
    "                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
    "            ],\n",
    "                     random_order=True\n",
    "        )\n",
    "    ],\n",
    "    random_order=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold crossvalidation with tiles in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#doesnt match the unshuffled numbers???\n",
    "i=0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    i=i+1\n",
    "    #slice training-data into folds for training/testing variables\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    #get .fit values\n",
    "    num_train_samples = len(X_train)\n",
    "    num_val_samples = len(X_valid)  \n",
    "    train_steps = np.ceil(num_train_samples // train_batch_size)\n",
    "    val_steps = np.ceil(num_val_samples // val_batch_size)\n",
    "    \n",
    "    #get data-generators\n",
    "    train_generator = datagen.flow(X_train, y_train, batch_size = train_batch_size)\n",
    "    val_generator = datagen_test.flow(X_valid, y_valid, batch_size = val_batch_size)\n",
    "    \n",
    "    #Callbacks & Model\n",
    "    name_weights = (base_path + \"\\Fold_\" + str(i) + \"{}.h5\" .format(model_name))\n",
    "    callbacks_list = get_callbacks(name_weights = name_weights)\n",
    "    model_VGG = get_model_classif_VGG_base_nottrainable()\n",
    " \n",
    "    #first on small batch_size to establish the basic informations\n",
    "    history = model_VGG.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=5, verbose=1,\n",
    "        validation_data = val_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks_list)\n",
    "\n",
    "    #second round --> whole model with larger batchsize\n",
    "    num_train_samples = len(X_train)\n",
    "    num_val_samples = len(X_valid)  \n",
    "    train_steps = np.ceil(num_train_samples // train_batch_size_2)\n",
    "    val_steps = np.ceil(num_val_samples // val_batch_size)\n",
    "    \n",
    "    #get data-generators\n",
    "    train_generator = datagen.flow(X_train, y_train, batch_size = train_batch_size_2)\n",
    "    val_generator = datagen_test.flow(X_valid, y_valid, batch_size = val_batch_size) \n",
    "    model_VGG_bt = get_model_classif_VGG_base_trainable()\n",
    "    model_VGG_bt.load_weights(name_weights)\n",
    "        \n",
    "    history = model_VGG_bt.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=50, verbose=1,\n",
    "        validation_data = val_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks_list)\n",
    "    \n",
    "    print(model_VGG.evaluate(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Folder Structure for tiles -> debugging purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data_frame for all data: Path, ID, Label\n",
    "tiles_path = r\"C:\\Users\\eg38emed\\FCD\\Tiles\\Train_crossvalidation\"\n",
    "\n",
    "fcd = tiles_path + '\\FCD'\n",
    "tsc = tiles_path + '\\TSC'\n",
    "\n",
    "#fcd folder\n",
    "df = pd.DataFrame({'path': glob(os.path.join(fcd,'*.png'))})\n",
    "df['id'] = df.path.map(lambda x: x.split('\\\\')[7].split('.png')[0]) \n",
    "df['label'] = 0\n",
    "\n",
    "#tsc folder\n",
    "df_pos = pd.DataFrame({'path': glob(os.path.join(tsc,'*.png'))})\n",
    "df_pos['id'] = df_pos.path.map(lambda x: x.split('\\\\')[7].split('.png')[0]) \n",
    "df_pos['label'] = 1\n",
    "\n",
    "#concate\n",
    "df_train = pd.concat([df, df_pos])\n",
    "\n",
    "#add images\n",
    "#df_train['image'] = df_train['path'].map(imread)\n",
    "\n",
    "print(len(df_train))\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dummy arrays for stratified-k-fold -> only number of items matters\n",
    "X = np.asarray(df_train['id'])\n",
    "y = np.asarray(df_train['label'])\n",
    "print(len(y))\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = 10\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=101, shuffle = False)\n",
    "\n",
    "train_batch_size_1 = 64\n",
    "val_batch_size = 64\n",
    "\n",
    "#datagenerators\n",
    "datagen_train = ImageDataGenerator(preprocessing_function=seq.augment_image,\n",
    "                            rescale=1./255)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new directory-structure --> flow_from_directory + better debugging\n",
    "file_path = (r'C:\\Users\\eg38emed\\FCD\\kfold')\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path)\n",
    "\n",
    "i=0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    i=i+1\n",
    "'''    y = to_categorical(y)\n",
    "    #slice training-data into folds for training/testing variables\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "'''\n",
    "    base_dir = (file_path + r'\\base_dir_fold_' + str(i))\n",
    "    print(base_dir)\n",
    "    os.mkdir(base_dir)\n",
    "    # train_dir\n",
    "    train_dir = os.path.join(base_dir, 'train_dir')\n",
    "    print(train_dir)\n",
    "    os.mkdir(train_dir)\n",
    "\n",
    "    # val_dir\n",
    "    val_dir = os.path.join(base_dir, 'val_dir')\n",
    "    os.mkdir(val_dir)\n",
    "\n",
    "    # Inside each folder we create seperate folders for each class\n",
    "    # create new folders inside train_dir\n",
    "    fcd = os.path.join(train_dir, 'a_fcd')\n",
    "    os.mkdir(fcd)\n",
    "    tsc = os.path.join(train_dir, 'b_tsc')\n",
    "    os.mkdir(tsc)\n",
    "\n",
    "    # create new folders inside val_dir\n",
    "    fcd = os.path.join(val_dir, 'a_fcd')\n",
    "    os.mkdir(fcd)\n",
    "    tsc = os.path.join(val_dir, 'b_tsc')\n",
    "    os.mkdir(tsc)\n",
    "\n",
    "    # check that the folders have been created\n",
    "    print(os.listdir(base_dir + '\\\\train_dir'))\n",
    "    print(os.listdir(base_dir + '\\\\val_dir'))    \n",
    "\n",
    "    # Transfer the train images\n",
    "    for j, image in enumerate(train_index):\n",
    "        # get the label for a certain image\n",
    "        print(image)\n",
    "        path = df_train.iloc[image, 0]\n",
    "        target = path.split('\\\\')[6]\n",
    "        print(path)\n",
    "        fname = df_train.iloc[image, 1]\n",
    "        fname = fname + '.png'\n",
    "        # these must match the folder names\n",
    "        if target == 'FCD':\n",
    "            label = 'a_fcd'\n",
    "        if target == 'TSC':\n",
    "            label = 'b_tsc'    \n",
    "        # source path to image\n",
    "        #src = os.path.join(path, fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(path, dst)\n",
    "\n",
    "\n",
    "    # Transfer the val images\n",
    "    for j, image in enumerate(test_index):\n",
    "       # get the label for a certain image\n",
    "        print('Val:', image)\n",
    "        path = df_train.iloc[image, 0]\n",
    "        target = path.split('\\\\')[6]\n",
    "        print(path)\n",
    "        fname = df_train.iloc[image, 1]\n",
    "        fname = fname + '.png'\n",
    "        # these must match the folder names\n",
    "        if target == 'FCD':\n",
    "            label = 'a_fcd'\n",
    "        if target == 'TSC':\n",
    "            label = 'b_tsc'   \n",
    "        # source path to image\n",
    "        #rc = os.path.join(path, fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(path, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold crossvalidation on folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-16 with BatchNorm + 2. Denselayer\n",
    "def get_model_classif_VGG_base_nottrainable():\n",
    "    base_model_VGG = applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    print('Model loaded.')\n",
    "    \n",
    "    for layer in base_model_VGG.layers:\n",
    "        layer.trainable = False\n",
    "    print(\"trainable: only toplayer\")\n",
    "    \n",
    "    x = base_model_VGG.output\n",
    "    out = GlobalMaxPooling2D(name='1')(x)\n",
    "    x = BatchNormalization(name='2')(out)\n",
    "    x = Dropout(0.5, name='3')(x)\n",
    "    x = Dense(512, activation='relu',name='4') (x)\n",
    "    x = BatchNormalization(name='5')(x)\n",
    "    x = Dropout(0.2, name='6')(x)\n",
    "    x = Dense(256, activation='relu',name='7') (x)\n",
    "    x = BatchNormalization(name='8')(x)\n",
    "    \n",
    "    predictions = (Dense(2, activation='softmax'))(x)\n",
    "    model_VGG = Model(inputs=base_model_VGG.input, outputs=predictions)\n",
    "\n",
    "    adam = optimizers.Adam()\n",
    "    model_VGG.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=[categorical_accuracy, auc_roc])\n",
    "\n",
    "    model_VGG.summary()\n",
    "    return model_VGG\n",
    "\n",
    "def get_model_classif_VGG_base_trainable():\n",
    "    base_model_VGG = applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    print('Model loaded.')\n",
    "    \n",
    "    for layer in base_model_VGG.layers:\n",
    "        layer.trainable = True\n",
    "        print(\"trainable: whole model\")\n",
    "    \n",
    "    x = base_model_VGG.output\n",
    "    out = GlobalMaxPooling2D(name='1')(x)\n",
    "    x = BatchNormalization(name='2')(out)\n",
    "    x = Dropout(0.5, name='3')(x)\n",
    "    x = Dense(512, activation='relu',name='4') (x)\n",
    "    x = BatchNormalization(name='5')(x)\n",
    "    x = Dropout(0.5, name='6')(x)\n",
    "    x = Dense(256, activation='relu',name='7') (x)\n",
    "    x = BatchNormalization(name='8')(x)\n",
    "    \n",
    "    predictions = (Dense(2, activation='softmax'))(x)\n",
    "    model_VGG = Model(inputs=base_model_VGG.input, outputs=predictions)\n",
    "\n",
    "    adam = optimizers.Adam()\n",
    "    model_VGG.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=[categorical_accuracy, auc_roc])\n",
    "\n",
    "    model_VGG.summary()\n",
    "    return model_VGG\n",
    "\n",
    "def get_callbacks_clr(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=15, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    tensorboard_callback = TensorBoard(log_dir=base_path + \"\\\\logs\\\\{}\".format(time()), \n",
    "            histogram_freq=0, #batch_size=32, \n",
    "            write_graph=True, write_grads=False, write_images=True, \n",
    "            embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "    clr = clr_callback.CyclicLR(base_lr=0.0001, max_lr=0.001,\n",
    "                        step_size=num_train_samples/train_batch_size_1/2)\n",
    "    csv_logger = CSVLogger(filename = base_path + \"\\\\model_history_{}_toplayer.csv\".format(model_name), append=True)\n",
    "    return [clr, earlystopping, ReduceLR, checkpoint, tensorboard_callback, csv_logger]\n",
    "\n",
    "def get_callbacks_clr_2(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=15, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    tensorboard_callback = TensorBoard(log_dir=base_path + \"\\\\logs\\\\{}\".format(time()), \n",
    "            histogram_freq=0, #batch_size=32, \n",
    "            write_graph=True, write_grads=False, write_images=True, \n",
    "            embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "    clr = clr_callback.CyclicLR(base_lr=0.000005, max_lr=0.00005,\n",
    "                        step_size=num_train_samples/train_batch_size_1/2)    \n",
    "    csv_logger = CSVLogger(filename = base_path + \"\\\\model_history_{}_ALL.csv\".format(model_name), append=True)\n",
    "    return [earlystopping, ReduceLR, checkpoint, tensorboard_callback,  csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model: VGG_kfold_3 on Fold: 3\n",
      "Found 4323 images belonging to 2 classes.\n",
      "Found 481 images belonging to 2 classes.\n",
      "Model loaded.\n",
      "trainable: only toplayer\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "1 (GlobalMaxPooling2D)       (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "2 (BatchNormalization)       (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "3 (Dropout)                  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "4 (Dense)                    (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "5 (BatchNormalization)       (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "6 (Dropout)                  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "7 (Dense)                    (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "8 (BatchNormalization)       (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 15,114,306\n",
      "Trainable params: 397,058\n",
      "Non-trainable params: 14,717,248\n",
      "_________________________________________________________________\n",
      "Epoch 1/13\n",
      " 52/680 [=>............................] - ETA: 30:38 - loss: 0.9033 - categorical_accuracy: 0.5329 - auc: 0.5206"
     ]
    }
   ],
   "source": [
    "#with CSV_Logger\n",
    "i = 2\n",
    "kfold = 10\n",
    "IMAGE_SIZE = 100\n",
    "IMAGE_CHANNELS = 3\n",
    "train_batch_size_1 = 128\n",
    "val_batch_size = 64\n",
    "\n",
    "while i <= kfold: \n",
    "    i +=1\n",
    "    \n",
    "    if i == 11:\n",
    "        break\n",
    "\n",
    "    model_name = ('VGG_kfold_{}'.format(i))\n",
    "    base_path = (r'C:\\Users\\eg38emed\\FCD\\kfold_RCZ\\base_dir_fold_{}'.format(i)) \n",
    "    print('Training Model: {} on Fold: {}'.format(model_name, i))\n",
    "    train_path = (base_path + '\\\\train_dir') \n",
    "    val_path = (base_path + '\\\\val_dir')\n",
    "\n",
    "    datagen_train = ImageDataGenerator(preprocessing_function=seq.augment_image,\n",
    "                                       rotate_random_zoom_crop=True,\n",
    "                                       rescale=1./255)\n",
    "\n",
    "    datagen_val = ImageDataGenerator(rescale=1./255,\n",
    "                                    rotate_random_zoom_crop=True)\n",
    "\n",
    "    train_gen_1 = DirectoryIterator(train_path, datagen_train,\n",
    "                                            #target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                            crop_size = (100, 100),\n",
    "                                            batch_size=train_batch_size_1,\n",
    "                                            class_mode='categorical')\n",
    "\n",
    "    val_gen = DirectoryIterator(val_path,\n",
    "                                datagen_val,\n",
    "                                #target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                crop_size = (100, 100),\n",
    "                                batch_size=val_batch_size,\n",
    "                                class_mode='categorical')\n",
    "    \n",
    "    num_train_samples = train_gen_1.samples\n",
    "    num_val_samples = val_gen.samples\n",
    "\n",
    "    #train toplayer with cyclic\n",
    "    model_VGG = get_model_classif_VGG_base_nottrainable()\n",
    "    name_weights = (base_path + \"\\\\{}_toplayer.h5\".format(model_name))\n",
    "    callbacks_list = get_callbacks_clr(name_weights = name_weights)\n",
    "    \n",
    "    history = model_VGG.fit_generator(train_gen_1, \n",
    "                                    steps_per_epoch=len(train_gen_1)*20, \n",
    "                                    validation_data=val_gen,\n",
    "                                    validation_steps=len(val_gen)*20,\n",
    "                                    epochs=13, verbose=1,\n",
    "                                    use_multiprocessing = False, workers = 6, \n",
    "                                    callbacks=callbacks_list)\n",
    "    plot_history(history, modelname = ('\\\\{}_toplayer'.format(model_name)), path=base_path)\n",
    "    \n",
    "    #FINETUNE ALL VGG with fixed LR\n",
    "    model_VGG = get_model_classif_VGG_base_trainable()\n",
    "    model_VGG.load_weights(base_path + \"\\\\{}_toplayer.h5\".format(model_name))\n",
    "    \n",
    "    name_weights = (base_path + \"\\\\{}_ALL.h5\".format(model_name))\n",
    "    callbacks_list = get_callbacks_clr_2(name_weights = name_weights)\n",
    "    \n",
    "    history = model_VGG.fit_generator(train_gen_1, \n",
    "                        steps_per_epoch=len(train_gen_1)*20, \n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=len(val_gen)*20,\n",
    "                        epochs=20, verbose=1,\n",
    "                        use_multiprocessing = False, workers = 6, \n",
    "                        callbacks=callbacks_list)\n",
    "    plot_history(history, modelname = ('\\\\{}_All'.format(model_name)), path=base_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain some Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1,2,3,4,6,7,8: >90% with 100x100, trained toplayer (0.5 DO, cyclic lr) -> trained model (0.5 DO, 0.0001 lr)\n",
    "#retrain later: 9: 82% 5: 84%\n",
    "#retraining some Models: 10: 65% all / 75% in with trained toplayer\n",
    "i = 3\n",
    "IMAGE_SIZE = 100\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "model_name = ('VGG_kfold_{}'.format(i))\n",
    "base_path = (r'C:\\Users\\eg38emed\\FCD\\kfold_RCZ\\base_dir_fold_{}'.format(i)) \n",
    "print('Training Model: {} on Fold: {}'.format(model_name, i))\n",
    "print(base_path)\n",
    "\n",
    "train_path = (base_path + '\\\\train_dir') \n",
    "val_path = (base_path + '\\\\val_dir')\n",
    "\n",
    "train_batch_size_1 = 128\n",
    "val_batch_size = 64\n",
    "\n",
    "datagen_train = ImageDataGenerator(preprocessing_function=seq.augment_image,\n",
    "                                       rotate_random_zoom_crop=True,\n",
    "                                       rescale=1./255)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255,\n",
    "                                rotate_random_zoom_crop=True)\n",
    "\n",
    "train_gen_1 = DirectoryIterator(train_path, datagen_train,\n",
    "                                        #target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                        crop_size = (100, 100),\n",
    "                                        batch_size=train_batch_size_1,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "val_gen = DirectoryIterator(val_path,\n",
    "                            datagen_val,\n",
    "                            #target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                            crop_size = (100, 100),\n",
    "                            batch_size=val_batch_size,\n",
    "                            class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN toplayer clr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train toplayer with cyclic\n",
    "model_VGG = get_model_classif_VGG_base_nottrainable()\n",
    "name_weights = (base_path + \"\\\\{}_toplayer.h5\".format(model_name))\n",
    "callbacks_list = get_callbacks_clr(name_weights = name_weights)\n",
    "\n",
    "history = model_VGG.fit_generator(train_gen_1, \n",
    "                                steps_per_epoch=len(train_gen_1)*20, \n",
    "                                validation_data=val_gen,\n",
    "                                validation_steps=len(val_gen)*20,\n",
    "                                epochs=13, verbose=1,\n",
    "                                use_multiprocessing = False, workers = 6, \n",
    "                                callbacks=callbacks_list)\n",
    "plot_history(history, modelname = ('\\\\{}_toplayer'.format(model_name)), path=base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FINETUNE ALL VGG with clr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINETUNE ALL VGG with fixed LR\n",
    "model_VGG = get_model_classif_VGG_base_trainable()\n",
    "model_VGG.load_weights(base_path + \"\\\\{}_toplayer.h5\".format(model_name))\n",
    "\n",
    "name_weights = (base_path + \"\\\\{}_ALL.h5\".format(model_name))\n",
    "callbacks_list = get_callbacks_clr_2(name_weights = name_weights)\n",
    "\n",
    "history = model_VGG.fit_generator(train_gen_1, \n",
    "                    steps_per_epoch=len(train_gen_1)*20, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=len(val_gen)*20,\n",
    "                    epochs=20, verbose=1,\n",
    "                    use_multiprocessing = False, workers = 6, \n",
    "                    callbacks=callbacks_list)\n",
    "plot_history(history, modelname = ('\\\\{}_All'.format(model_name)), path=base_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
