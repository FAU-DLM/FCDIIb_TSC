{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold-Crossvalidation with a VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "- get validation process straight\n",
    "- hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "print(os.listdir(\"/home/Deep_Learner/work/local/histopathologic-cancer-detection\"))\n",
    "\n",
    "from glob import glob \n",
    "from skimage.io import imread\n",
    "import gc\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras import applications, regularizers, optimizers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#visualize Augmentation from directory!\n",
    "def looking_at_augmentation (data_generator, batchsize, path):\n",
    "    im, label = next(data_generator)\n",
    "    print(im)\n",
    "    im = (im - np.min(im))/np.ptp(im) # to normalize all images --> matplotlib only takes pos. values between 0..1 / 0..255 \n",
    "    print(im)\n",
    "    imgs = list(im)\n",
    "    labels = list(label)\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.suptitle('Augmented Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, im  in enumerate(imgs[:batchsize]):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(im)\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "    #fig.tight_layout()\n",
    "    fig.savefig(base_path + '\\\\Augmented-Images.png', dpi=300)\n",
    "\n",
    "#fast plot of training history\n",
    "def plot_history(history, modelname, path):\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n",
    "    axs[0].plot(hist_df.val_categorical_accuracy, lw=5, label='Validation Accuracy')\n",
    "    axs[0].plot(hist_df.categorical_accuracy, lw=5, label='Training Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].grid()\n",
    "    axs[0].legend(loc=0)\n",
    "    axs[1].plot(hist_df.val_loss, lw=5, label='Validation MLogLoss')\n",
    "    axs[1].plot(hist_df.loss, lw=5, label='Training MLogLoss')\n",
    "    axs[1].set_ylabel('MLogLoss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].grid()\n",
    "    axs[1].legend(loc=0)\n",
    "    fig.savefig(path + '\\History_{}.png' .format(modelname), dpi=300)\n",
    "    hist_df.to_csv()\n",
    "    plt.show();\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc#plotting the receiver operating characteristics --> evaluate performance cutting point vice\n",
    "def plot_roc(label, predictions, modelname, path): #IDEA: set diffrent cutting point based on ROC for ensembling   \n",
    "    roc_auc_score(label, predictions)\n",
    "    print('The ROC-Score is: {}' .format(roc_auc_score))\n",
    "\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(label, predictions)\n",
    "    auc_keras = auc(fpr_keras, tpr_keras)\n",
    "    #print(auc_keras)\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve: {}' .format(auc_keras))\n",
    "    plt.legend(loc='best')\n",
    "    fig.savefig(path + '\\ROC-Curve_{}.png' .format(modelname), dpi=300) #saving PLOT \n",
    "    plt.show()\n",
    "    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "# Source: Scikit Learn website\n",
    "# http://scikit-learn.org/stable/auto_examples/\n",
    "# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n",
    "# selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "#plotting correctly classified images: https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python\n",
    "def plot_correct(vals, y_pred, y_label, modelname, path):\n",
    "    correct = np.where(y_pred==y_label)[0]\n",
    "    print (\"Found %d correct labels\" % len(correct))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.suptitle('Correct Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, correct in enumerate(correct[:9]):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(vals[correct])\n",
    "        ax.set_title(\"Predicted {}, Class {}\".format(y_pred[correct], y_label[correct]), fontsize=10)\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "    fig.savefig(path + '\\Correct_Images_{}.png' .format(modelname), dpi=100) #saving PLOT \n",
    "\n",
    "#Plotting incorrectly classified\n",
    "def plot_incorrect(vals, y_pred, y_label, modelname, path):\n",
    "    incorrect = np.where(y_pred!=y_label)[0]\n",
    "    print (\"Found %d incorrect labels\" % len(incorrect))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "    plt.suptitle('Incorrect Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, incorrect in enumerate(incorrect[:9]):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(vals[incorrect])\n",
    "        ax.set_title(\"Predicted {}, Class {}\".format(y_pred[incorrect], y_label[incorrect]), fontsize=10)\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "    fig.savefig(path + '\\Incorrect_Images_{}.png' .format(modelname), dpi=100) #saving PLOT \n",
    "    \n",
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value\n",
    "    \n",
    "from scipy.misc import imread\n",
    "#list of tiffs to array to plot_correct / incorrect-images + rounding predictions to compare to labels\n",
    "def prep_im_label (val, y_pred):\n",
    "    vals = []\n",
    "    for i in val:\n",
    "        vals.append(imread(i))\n",
    "        #q = q+1\n",
    "        #print(q)\n",
    "        #print(i)\n",
    "    vals = np.asarray(vals)    \n",
    "    print(vals.shape)\n",
    "    y_pred = np.round(y_pred)\n",
    "    return(vals, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        # apply the following augmenters to most images\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "        iaa.Flipud(0.5), # vertically flip 20% of all images\n",
    "        sometimes(iaa.Affine(\n",
    "            scale={\"x\": (0.9, 1.6), \"y\": (0.9, 1.6)}, #>20 will cut part of img\n",
    "            translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)}, # >20% will also cut part of img\n",
    "            rotate=(-10, 10), # 45/-45° -> works good with scale + translate to prevent cuts\n",
    "            shear=(-5, 5), # shear by -16 to +16 degrees\n",
    "            mode=ia.ALL \n",
    "        )),\n",
    "        iaa.SomeOf((0, 4), [\n",
    "                sometimes(iaa.Superpixels(p_replace=(0.3, 0.7), n_segments=(10, 100))), #superpixel-representation --> better basallamina representation \n",
    "                iaa.OneOf([\n",
    "                    iaa.GaussianBlur((0, 0.2)), #small blur effects --> better representation\n",
    "                    iaa.AverageBlur(k=(1, 3)), # k must be odd\n",
    "                    iaa.MedianBlur(k=(1, 3)), # \n",
    "                ]),\n",
    "                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)), #cell wall represenation\n",
    "                iaa.Emboss(alpha=(0, 0.8), strength=(0, 0.5)), #cell wall represenation\n",
    "                #searching for edges or angles --> blobby mask --> better basallamina representation / nuclei\n",
    "                iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                    iaa.EdgeDetect(alpha=(0.2, 0.4)), #detects edges --> cell wall,..\n",
    "                    iaa.DirectedEdgeDetect(alpha=(0.2, 0.4), direction=(0.0, 1.0)), #direction will make edges from random directions \n",
    "                ])),\n",
    "                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.2), # add gaussian noise to images\n",
    "             iaa.OneOf([\n",
    "                    iaa.Dropout((0.05, 0.3), per_channel=0.2), #rnd remove 5% in small pixels\n",
    "                    iaa.CoarseDropout((0.05, 0.3), size_percent=(0.01, 0.02), per_channel=0.2),# rnd remove 3% in big pixels\n",
    "                ]),\n",
    "                iaa.Invert(0.01, per_channel=True), # invert color channels\n",
    "                iaa.Add((-10, 10), per_channel=0.3), # change brightness of images (by -10 to 10 of original value)\n",
    "                #iaa.AddToHueAndSaturation((-0.1, 0.1)), # change hue and saturation\n",
    "                #\n",
    "                #either change the brightness of the whole image (sometimes per channel) or change the brightness of subareas\n",
    "                iaa.OneOf([\n",
    "                    iaa.Multiply((0.9, 1.2), per_channel=0.5),\n",
    "                    iaa.FrequencyNoiseAlpha(\n",
    "                        exponent=(-1, 0),\n",
    "                        first=iaa.Multiply((0.9, 1.1), per_channel=True),\n",
    "                        second=iaa.ContrastNormalization((0.9, 1.1))\n",
    "                    )\n",
    "                ]),\n",
    "                sometimes(iaa.ElasticTransformation(alpha=(0, 0.5), sigma=0.1)), #still not sure: move pixels locally around\n",
    "                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.03))), #still not sure:move parts of the image around\n",
    "                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
    "            ],\n",
    "                     random_order=True\n",
    "        )\n",
    "    ],\n",
    "    random_order=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold crossvalidation with tiles in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#doesnt match the unshuffled numbers???\n",
    "i=0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    i=i+1\n",
    "    #slice training-data into folds for training/testing variables\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    #get .fit values\n",
    "    num_train_samples = len(X_train)\n",
    "    num_val_samples = len(X_valid)  \n",
    "    train_steps = np.ceil(num_train_samples // train_batch_size)\n",
    "    val_steps = np.ceil(num_val_samples // val_batch_size)\n",
    "    \n",
    "    #get data-generators\n",
    "    train_generator = datagen.flow(X_train, y_train, batch_size = train_batch_size)\n",
    "    val_generator = datagen_test.flow(X_valid, y_valid, batch_size = val_batch_size)\n",
    "    \n",
    "    #Callbacks & Model\n",
    "    name_weights = (base_path + \"\\Fold_\" + str(i) + \"{}.h5\" .format(model_name))\n",
    "    callbacks_list = get_callbacks(name_weights = name_weights)\n",
    "    model_VGG = get_model_classif_VGG_base_nottrainable()\n",
    " \n",
    "    #first on small batch_size to establish the basic informations\n",
    "    history = model_VGG.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=5, verbose=1,\n",
    "        validation_data = val_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks_list)\n",
    "\n",
    "    #second round --> whole model with larger batchsize\n",
    "    num_train_samples = len(X_train)\n",
    "    num_val_samples = len(X_valid)  \n",
    "    train_steps = np.ceil(num_train_samples // train_batch_size_2)\n",
    "    val_steps = np.ceil(num_val_samples // val_batch_size)\n",
    "    \n",
    "    #get data-generators\n",
    "    train_generator = datagen.flow(X_train, y_train, batch_size = train_batch_size_2)\n",
    "    val_generator = datagen_test.flow(X_valid, y_valid, batch_size = val_batch_size) \n",
    "    model_VGG_bt = get_model_classif_VGG_base_trainable()\n",
    "    model_VGG_bt.load_weights(name_weights)\n",
    "        \n",
    "    history = model_VGG_bt.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=50, verbose=1,\n",
    "        validation_data = val_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks_list)\n",
    "    \n",
    "    print(model_VGG.evaluate(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Folder Structure for tiles -> debugging purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data_frame for all data: Path, ID, Label\n",
    "tiles_path = r\"C:\\Users\\eg38emed\\FCD\\Tiles\\Train_crossvalidation\"\n",
    "\n",
    "fcd = tiles_path + '\\FCD'\n",
    "tsc = tiles_path + '\\TSC'\n",
    "\n",
    "#fcd folder\n",
    "df = pd.DataFrame({'path': glob(os.path.join(fcd,'*.png'))})\n",
    "df['id'] = df.path.map(lambda x: x.split('\\\\')[7].split('.png')[0]) \n",
    "df['label'] = 0\n",
    "\n",
    "#tsc folder\n",
    "df_pos = pd.DataFrame({'path': glob(os.path.join(tsc,'*.png'))})\n",
    "df_pos['id'] = df_pos.path.map(lambda x: x.split('\\\\')[7].split('.png')[0]) \n",
    "df_pos['label'] = 1\n",
    "\n",
    "#concate\n",
    "df_train = pd.concat([df, df_pos])\n",
    "\n",
    "#add images\n",
    "#df_train['image'] = df_train['path'].map(imread)\n",
    "\n",
    "print(len(df_train))\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dummy arrays for stratified-k-fold -> only number of items matters\n",
    "X = np.asarray(df_train['id'])\n",
    "y = np.asarray(df_train['label'])\n",
    "print(len(y))\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = 10\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=101, shuffle = False)\n",
    "\n",
    "train_batch_size_1 = 64\n",
    "val_batch_size = 64\n",
    "\n",
    "#datagenerators\n",
    "datagen_train = ImageDataGenerator(preprocessing_function=seq.augment_image,\n",
    "                            rescale=1./255)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new directory-structure --> flow_from_directory + better debugging\n",
    "file_path = (r'C:\\Users\\eg38emed\\FCD\\kfold')\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path)\n",
    "\n",
    "i=0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    i=i+1\n",
    "'''    y = to_categorical(y)\n",
    "    #slice training-data into folds for training/testing variables\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "'''\n",
    "    base_dir = (file_path + r'\\base_dir_fold_' + str(i))\n",
    "    print(base_dir)\n",
    "    os.mkdir(base_dir)\n",
    "    # train_dir\n",
    "    train_dir = os.path.join(base_dir, 'train_dir')\n",
    "    print(train_dir)\n",
    "    os.mkdir(train_dir)\n",
    "\n",
    "    # val_dir\n",
    "    val_dir = os.path.join(base_dir, 'val_dir')\n",
    "    os.mkdir(val_dir)\n",
    "\n",
    "    # Inside each folder we create seperate folders for each class\n",
    "    # create new folders inside train_dir\n",
    "    fcd = os.path.join(train_dir, 'a_fcd')\n",
    "    os.mkdir(fcd)\n",
    "    tsc = os.path.join(train_dir, 'b_tsc')\n",
    "    os.mkdir(tsc)\n",
    "\n",
    "    # create new folders inside val_dir\n",
    "    fcd = os.path.join(val_dir, 'a_fcd')\n",
    "    os.mkdir(fcd)\n",
    "    tsc = os.path.join(val_dir, 'b_tsc')\n",
    "    os.mkdir(tsc)\n",
    "\n",
    "    # check that the folders have been created\n",
    "    print(os.listdir(base_dir + '\\\\train_dir'))\n",
    "    print(os.listdir(base_dir + '\\\\val_dir'))    \n",
    "\n",
    "    # Transfer the train images\n",
    "    for j, image in enumerate(train_index):\n",
    "        # get the label for a certain image\n",
    "        print(image)\n",
    "        path = df_train.iloc[image, 0]\n",
    "        target = path.split('\\\\')[6]\n",
    "        print(path)\n",
    "        fname = df_train.iloc[image, 1]\n",
    "        fname = fname + '.png'\n",
    "        # these must match the folder names\n",
    "        if target == 'FCD':\n",
    "            label = 'a_fcd'\n",
    "        if target == 'TSC':\n",
    "            label = 'b_tsc'    \n",
    "        # source path to image\n",
    "        #src = os.path.join(path, fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(path, dst)\n",
    "\n",
    "\n",
    "    # Transfer the val images\n",
    "    for j, image in enumerate(test_index):\n",
    "       # get the label for a certain image\n",
    "        print('Val:', image)\n",
    "        path = df_train.iloc[image, 0]\n",
    "        target = path.split('\\\\')[6]\n",
    "        print(path)\n",
    "        fname = df_train.iloc[image, 1]\n",
    "        fname = fname + '.png'\n",
    "        # these must match the folder names\n",
    "        if target == 'FCD':\n",
    "            label = 'a_fcd'\n",
    "        if target == 'TSC':\n",
    "            label = 'b_tsc'   \n",
    "        # source path to image\n",
    "        #rc = os.path.join(path, fname)\n",
    "        # destination path to image\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        # copy the image from the source to the destination\n",
    "        shutil.copyfile(path, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold crossvalidation on folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-16 for heatmap generation \n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "def get_model_classif_VGG_base_nottrainable():\n",
    "    base_model_VGG = applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    print('Model loaded.')\n",
    "    \n",
    "    for layer in base_model_VGG.layers:\n",
    "        layer.trainable = False\n",
    "        print(\"trainable:\", layer.name)\n",
    "    \n",
    "    x = base_model_VGG.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = (Dense(256, activation='relu'))(x)\n",
    "    x = (Dropout(0.2)) (x)\n",
    "    predictions = (Dense(2, activation='softmax'))(x)\n",
    "    model_VGG = Model(inputs=base_model_VGG.input, outputs=predictions)\n",
    "\n",
    "    adam = optimizers.Adam()\n",
    "    model_VGG.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=[categorical_accuracy, auc_roc])\n",
    "\n",
    "    model_VGG.summary()\n",
    "    return model_VGG\n",
    "\n",
    "def get_model_classif_VGG_base_trainable():\n",
    "    base_model_VGG = applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    print('Model loaded.')\n",
    "    \n",
    "    for layer in base_model_VGG.layers:\n",
    "        layer.trainable = True\n",
    "        print(\"trainable:\", layer.name)\n",
    "    \n",
    "    x = base_model_VGG.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = (Dense(256, activation='relu'))(x)\n",
    "    x = (Dropout(0.5)) (x)\n",
    "    predictions = (Dense(2, activation='softmax'))(x)\n",
    "    model_VGG = Model(inputs=base_model_VGG.input, outputs=predictions)\n",
    "\n",
    "    adam = optimizers.Adam()\n",
    "    model_VGG.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=[categorical_accuracy, auc_roc])\n",
    "\n",
    "    model_VGG.summary()\n",
    "    return model_VGG\n",
    "\n",
    "import clr_callback\n",
    "def get_callbacks_clr(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=15, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    tensorboard_callback = TensorBoard(log_dir=base_path+ \"logs\\\\{}\".format(time()), \n",
    "            histogram_freq=0, #batch_size=32, \n",
    "            write_graph=True, write_grads=False, write_images=True, \n",
    "            embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "    clr = clr_callback.CyclicLR(base_lr=0.0001, max_lr=0.0005,\n",
    "                        step_size=num_train_samples/train_batch_size_1/2)\n",
    "    csv_logger = CSVLogger(filename = base_path + \"model_history_{}_toplayer.csv\".format(modelname), append=True)\n",
    "    return [clr, earlystopping, ReduceLR, checkpoint, tensorboard_callback, csv_logger]\n",
    "\n",
    "def get_callbacks(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=15, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    tensorboard_callback = TensorBoard(log_dir=base_path+ \"logs\\\\{}\".format(time()), \n",
    "            histogram_freq=0, #batch_size=32, \n",
    "            write_graph=True, write_grads=False, write_images=True, \n",
    "            embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "    csv_logger = CSVLogger(filename = base_path + \"model_history_{}_ALL.csv\".format(modelname), append=True)\n",
    "    return [earlystopping, ReduceLR, checkpoint, tensorboard_callback,  csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with CSV_Logger\n",
    "i = 0\n",
    "kfold = 10\n",
    "while i <= kfold: \n",
    "    i +=1\n",
    "    if i == 11:\n",
    "        break\n",
    "    IMAGE_SIZE = 200\n",
    "    IMAGE_CHANNELS = 3\n",
    "    model_name = ('VGG_kfold_{}'.format(i))\n",
    "   \n",
    "    base_path = (r'C:\\Users\\eg38emed\\FCD\\kfold\\base_dir_fold_{}\\\\'.format(i)) \n",
    "    print('Training Model: {} on Fold: {}'.format(model_name, i))\n",
    "    train_path = (base_path + 'train_dir') \n",
    "    val_path = (base_path + 'val_dir')\n",
    "\n",
    "    train_batch_size_1 = 128\n",
    "    val_batch_size = 64\n",
    "\n",
    "    datagen_train = ImageDataGenerator(preprocessing_function=seq.augment_image,\n",
    "                                rescale=1./255)\n",
    "\n",
    "    datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "    train_gen_1 = datagen_train.flow_from_directory(train_path,\n",
    "                                            target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                            batch_size=train_batch_size_1,\n",
    "                                            class_mode='categorical')\n",
    "\n",
    "    val_gen = datagen_val.flow_from_directory(val_path,\n",
    "                                            target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                            batch_size=val_batch_size,\n",
    "                                            class_mode='categorical')\n",
    "\n",
    "    #train toplayer with cyclic\n",
    "    model_VGG = get_model_classif_VGG_base_nottrainable()\n",
    "    name_weights = (base_path + \"\\{}.h5\".format(model_name))\n",
    "    callbacks_list = get_callbacks_clr(name_weights = name_weights)\n",
    "    \n",
    "    history = model_VGG.fit_generator(train_gen_1, \n",
    "                    steps_per_epoch=train_gen_1.samples // train_batch_size_1, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // val_batch_size,\n",
    "                    epochs=10, verbose=1,\n",
    "                    callbacks=callbacks_list)\n",
    "    plot_history(history, modelname = ('{}_toplayer'.format(model_name)), path=base_path)\n",
    "    \n",
    "    #FINETUNE ALL VGG with fixed LR\n",
    "    model_VGG = get_model_classif_VGG_base_trainable()\n",
    "    model_VGG.load_weights(base_path + \"\\{}.h5\".format(model_name))\n",
    "    name_weights = (base_path + \"\\{}_ALL.h5\".format(model_name))\n",
    "    callbacks_list = get_callbacks(name_weights = name_weights)\n",
    "    \n",
    "    history = model_VGG.fit_generator(train_gen_1, \n",
    "                    steps_per_epoch=train_gen_1.samples // train_batch_size_1, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // val_batch_size,\n",
    "                    epochs=30, verbose=1,\n",
    "                    callbacks=callbacks_list)\n",
    "    plot_history(history, modelname = ('{}_All'.format(model_name)), path=base_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain some Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1,2,3,4,6,7,8: >90% with 100x100, trained toplayer (0.5 DO, cyclic lr) -> trained model (0.5 DO, 0.0001 lr)\n",
    "#retrain later: 9: 82% 5: 84%\n",
    "#retraining some Models: 10: 65% all / 75% in with trained toplayer\n",
    "i = 4\n",
    "IMAGE_SIZE = 272\n",
    "IMAGE_CHANNELS = 3\n",
    "model_name = ('VGG_kfold_{}'.format(i))\n",
    "\n",
    "base_path = (r'C:\\Users\\eg38emed\\FCD\\kfold\\base_dir_fold_{}\\\\'.format(i)) \n",
    "print('Training Model: {} on Fold: {}'.format(model_name, i))\n",
    "print(base_path)\n",
    "train_path = (base_path + 'train_dir') \n",
    "val_path = (base_path + 'val_dir')\n",
    "\n",
    "train_batch_size_1 = 64\n",
    "val_batch_size = 64\n",
    "\n",
    "datagen_train = ImageDataGenerator(preprocessing_function=seq.augment_image,\n",
    "                            rescale=1./255)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen_1 = datagen_train.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=train_batch_size_1,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "val_gen = datagen_val.flow_from_directory(val_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train toplayer with cyclic\n",
    "model_VGG = get_model_classif_VGG_base_nottrainable()\n",
    "#model_VGG.load_weights(base_path + \"{}.h5\".format(model_name))\n",
    "name_weights = (base_path + \"{}.h5\".format(model_name))\n",
    "callbacks_list = get_callbacks_clr(name_weights = name_weights)\n",
    "\n",
    "history = model_VGG.fit_generator(train_gen_1, \n",
    "                steps_per_epoch=train_gen_1.samples // train_batch_size_1, \n",
    "                validation_data=val_gen,\n",
    "                validation_steps=val_gen.samples // val_batch_size,\n",
    "                epochs=10, verbose=1,\n",
    "                callbacks=callbacks_list)\n",
    "plot_history(history, modelname = ('{}_toplayer'.format(model_name)), path=base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINETUNE ALL VGG with fixed LR\n",
    "model_VGG = get_model_classif_VGG_base_trainable()\n",
    "model_VGG.load_weights(base_path + \"{}_ALL.h5\".format(model_name))\n",
    "name_weights = (base_path + \"{}_ALL_272_CLR.h5\".format(model_name))\n",
    "callbacks_list = get_callbacks_clr(name_weights = name_weights)\n",
    "\n",
    "history = model_VGG.fit_generator(train_gen_1, \n",
    "                steps_per_epoch=train_gen_1.samples // train_batch_size_1, \n",
    "                validation_data=val_gen,\n",
    "                validation_steps=val_gen.samples // val_batch_size,\n",
    "                epochs=4, verbose=1,\n",
    "                callbacks=callbacks_list)\n",
    "\n",
    "plot_history(history, modelname = ('{}_All_272_CLR'.format(model_name)), path=base_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
