{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#visualize Augmentation from directory!\n",
    "def looking_at_augmentation (data_generator, batchsize, path):\n",
    "    im, label = next(data_generator)\n",
    "    print(im)\n",
    "    im = (im - np.min(im))/np.ptp(im) # to normalize all images --> matplotlib only takes pos. values between 0..1 / 0..255 \n",
    "    print(im)\n",
    "    imgs = list(im)\n",
    "    labels = list(label)\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.suptitle('Augmented Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, im  in enumerate(imgs[:batchsize]):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(im)\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "    #fig.tight_layout()\n",
    "    fig.savefig(base_path + '\\\\Augmented-Images.png', dpi=300)\n",
    "\n",
    "#fast plot of training history\n",
    "def plot_history(history, modelname, path):\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n",
    "    axs[0].plot(hist_df.val_categorical_accuracy, lw=5, label='Validation Accuracy')\n",
    "    axs[0].plot(hist_df.categorical_accuracy, lw=5, label='Training Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].grid()\n",
    "    axs[0].legend(loc=0)\n",
    "    axs[1].plot(hist_df.val_loss, lw=5, label='Validation MLogLoss')\n",
    "    axs[1].plot(hist_df.loss, lw=5, label='Training MLogLoss')\n",
    "    axs[1].set_ylabel('MLogLoss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].grid()\n",
    "    axs[1].legend(loc=0)\n",
    "    fig.savefig(path + '\\History_{}.png' .format(modelname), dpi=300)\n",
    "    hist_df.to_csv()\n",
    "    plt.show();\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc#plotting the receiver operating characteristics --> evaluate performance cutting point vice\n",
    "def plot_roc(label, predictions, modelname, path): #IDEA: set diffrent cutting point based on ROC for ensembling   \n",
    "    roc_auc_score(label, predictions)\n",
    "    print('The ROC-Score is: {}' .format(roc_auc_score))\n",
    "\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(label, predictions)\n",
    "    auc_keras = auc(fpr_keras, tpr_keras)\n",
    "    #print(auc_keras)\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve: {}' .format(auc_keras))\n",
    "    plt.legend(loc='best')\n",
    "    fig.savefig(path + '\\ROC-Curve_{}.png' .format(modelname), dpi=300) #saving PLOT \n",
    "    plt.show()\n",
    "    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "# Source: Scikit Learn website\n",
    "# http://scikit-learn.org/stable/auto_examples/\n",
    "# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n",
    "# selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "#plotting correctly classified images: https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python\n",
    "def plot_correct(vals, y_pred, y_label, modelname, path):\n",
    "    correct = np.where(y_pred==y_label)[0]\n",
    "    print (\"Found %d correct labels\" % len(correct))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.suptitle('Correct Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, correct in enumerate(correct[:9]):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(vals[correct])\n",
    "        ax.set_title(\"Predicted {}, Class {}\".format(y_pred[correct], y_label[correct]), fontsize=10)\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "    fig.savefig(path + '\\Correct_Images_{}.png' .format(modelname), dpi=100) #saving PLOT \n",
    "\n",
    "#Plotting incorrectly classified\n",
    "def plot_incorrect(vals, y_pred, y_label, modelname, path):\n",
    "    incorrect = np.where(y_pred!=y_label)[0]\n",
    "    print (\"Found %d incorrect labels\" % len(incorrect))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3)\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "    plt.suptitle('Incorrect Images', fontsize=16)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    for ax in ax.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i, incorrect in enumerate(incorrect[:9]):\n",
    "        ax = fig.add_subplot(3,3,i+1)\n",
    "        ax.imshow(vals[incorrect])\n",
    "        ax.set_title(\"Predicted {}, Class {}\".format(y_pred[incorrect], y_label[incorrect]), fontsize=10)\n",
    "        fig.set_figheight(8)\n",
    "        fig.set_figwidth(8)\n",
    "\n",
    "    fig.savefig(path + '\\Incorrect_Images_{}.png' .format(modelname), dpi=100) #saving PLOT \n",
    "    \n",
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "print(os.listdir('/home/Deep_Learner/work/local/Tiles_272/Validation'))\n",
    "\n",
    "from glob import glob \n",
    "from skimage.io import imread\n",
    "import gc\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical # get categorical labels\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras import backend\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Concatenate, BatchNormalization, GlobalMaxPooling2D, GlobalAveragePooling2D, \\\n",
    "    Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint, Callback, LambdaCallback, CSVLogger\n",
    "from keras.metrics import categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        # apply the following augmenters to most images\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "        iaa.Flipud(0.5), # vertically flip 20% of all images\n",
    "        sometimes(iaa.Affine(\n",
    "            scale={\"x\": (0.9, 1.6), \"y\": (0.9, 1.6)}, #>20 will cut part of img\n",
    "            translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)}, # >20% will also cut part of img\n",
    "            rotate=(-10, 10), # 45/-45° -> works good with scale + translate to prevent cuts\n",
    "            shear=(-5, 5), # shear by -16 to +16 degrees\n",
    "            mode=ia.ALL \n",
    "        )),\n",
    "        iaa.SomeOf((0, 4), [\n",
    "                sometimes(iaa.Superpixels(p_replace=(0.3, 0.7), n_segments=(10, 100))), #superpixel-representation --> better basallamina representation \n",
    "                iaa.OneOf([\n",
    "                    iaa.GaussianBlur((0, 0.2)), #small blur effects --> better representation\n",
    "                    iaa.AverageBlur(k=(1, 3)), # k must be odd\n",
    "                    iaa.MedianBlur(k=(1, 3)), # \n",
    "                ]),\n",
    "                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)), #cell wall represenation\n",
    "                iaa.Emboss(alpha=(0, 0.8), strength=(0, 0.5)), #cell wall represenation\n",
    "                #searching for edges or angles --> blobby mask --> better basallamina representation / nuclei\n",
    "                iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                    iaa.EdgeDetect(alpha=(0.2, 0.4)), #detects edges --> cell wall,..\n",
    "                    iaa.DirectedEdgeDetect(alpha=(0.2, 0.4), direction=(0.0, 1.0)), #direction will make edges from random directions \n",
    "                ])),\n",
    "                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.2), # add gaussian noise to images\n",
    "             iaa.OneOf([\n",
    "                    iaa.Dropout((0.05, 0.3), per_channel=0.2), #rnd remove 5% in small pixels\n",
    "                    iaa.CoarseDropout((0.05, 0.3), size_percent=(0.01, 0.02), per_channel=0.2),# rnd remove 3% in big pixels\n",
    "                ]),\n",
    "                iaa.Invert(0.01, per_channel=True), # invert color channels\n",
    "                iaa.Add((-10, 10), per_channel=0.3), # change brightness of images (by -10 to 10 of original value)\n",
    "                #iaa.AddToHueAndSaturation((-0.1, 0.1)), # change hue and saturation\n",
    "                #\n",
    "                #either change the brightness of the whole image (sometimes per channel) or change the brightness of subareas\n",
    "                iaa.OneOf([\n",
    "                    iaa.Multiply((0.9, 1.2), per_channel=0.5),\n",
    "                    iaa.FrequencyNoiseAlpha(\n",
    "                        exponent=(-1, 0),\n",
    "                        first=iaa.Multiply((0.9, 1.1), per_channel=True),\n",
    "                        second=iaa.ContrastNormalization((0.9, 1.1))\n",
    "                    )\n",
    "                ]),\n",
    "                sometimes(iaa.ElasticTransformation(alpha=(0, 0.5), sigma=0.1)), #still not sure: move pixels locally around\n",
    "                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.03))), #still not sure:move parts of the image around\n",
    "                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
    "            ],\n",
    "                     random_order=True\n",
    "        )\n",
    "    ],\n",
    "    random_order=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 100\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "model_name = 'VGG_newtoplayer'\n",
    "\n",
    "base_path = ('/home/Deep_Learner/work/local/Models/{}'.format(model_name))\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "train_path = ('/home/Deep_Learner/work/local/Tiles_272/Validation')\n",
    "val_path = ('/home/Deep_Learner/work/local/Tiles_272/Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size_1 = 128\n",
    "val_batch_size = 64\n",
    "\n",
    "datagen_train = ImageDataGenerator(preprocessing_function=seq.augment_image,\n",
    "                            rescale=1./255)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "datagen_no_aug = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen_1 = datagen_train.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=train_batch_size_1,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "train_gen_noaug = datagen_train.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=train_batch_size_1,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "val_gen = datagen_val.flow_from_directory(val_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "num_train_samples = train_gen_1.samples\n",
    "num_val_samples = val_gen.samples\n",
    "print(num_train_samples)\n",
    "print(num_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = len(train_gen_noaug)\n",
    "nb_validation_samples = len(val_gen)\n",
    "\n",
    "batches = train_generator\n",
    "val_batches = validation_generator\n",
    "test_batches = test_generator\n",
    "steps_per_epoch = int(np.ceil(batches.samples/batch_size))\n",
    "validation_steps = int(np.ceil(val_batches.samples/batch_size))\n",
    "test_steps = int(np.ceil(test_batches.samples/batch_size))\n",
    "\n",
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "test_classes = test_batches.classes\n",
    "val_labels = to_categorical(val_classes)\n",
    "trn_labels = to_categorical(trn_classes)\n",
    "test_labels = to_categorical(test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=IMAGE_SIZE, IMAGE_SIZE, 3\n",
    "model_input = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_VGG = applications.VGG16(weights='imagenet', include_top=False, input_tensor=model_input, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = base_model.predict_generator(val_batches, validation_steps)\n",
    "\n",
    "trn_features = base_model.predict_generator(batches, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "save_array(model_path + 'train_base_model_xception_2_features.bc', trn_features)\n",
    "save_array(model_path + 'valid_base_model_xception_2_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_features = load_array(model_path+'train_base_model_xception_2_features.bc')\n",
    "val_features = load_array(model_path+'valid_base_model_xception_2_features.bc')\n",
    "\n",
    "trn_features.shape\n",
    "\n",
    "base_model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_model_tr():\n",
    "    inputs = Input(shape=base_model.output_shape[1:])\n",
    "    x = GlobalMaxPooling2D(name='1')(x)\n",
    "    x = BatchNormalization(name='5')(out)\n",
    "    x = Dropout(0.5, name='6')(x)\n",
    "    x = Dense(512, activation='relu',name='7') (x)\n",
    "    x = BatchNormalization(name='8')(x)\n",
    "    x = Dropout(0.2, name='9')(x)\n",
    "    x = Dense(256, activation='relu',name='10') (x)\n",
    "    x = BatchNormalization(name='11')(x)\n",
    "    \n",
    "    predictions = (Dense(2, activation='softmax'))(x)\n",
    "    model_VGG = Model(inputs=inputs, outputs=predictions)\n",
    "    \n",
    "    return top_model_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model_ttr= top_model_tr()\n",
    "top_model_ttr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in top_model_ttr.layers:\n",
    "    print(layer.name)\n",
    "for layer in top_model_ttr.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "opt = optimizers.Adam()\n",
    "top_model_ttr.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[categorical_accuracy, auc_roc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lr_train,y_lr_train= trn_features[:4000],  trn_labels[:4000]\n",
    "x_lr_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is a Keras model\n",
    "lr_finder_top_model_ttr = LRFinder(top_model_ttr)\n",
    "\n",
    "lr_finder_top_model_ttr.find(x_lr_train, y_lr_train, 0.000001, 1, batch_size=200, epochs=4)\n",
    "\n",
    "lr_finder_top_model_ttr.plot_loss(n_skip_beginning=1, n_skip_end=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clr_callback\n",
    "def get_callbacks_clr(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=10, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    clr = clr_callback.CyclicLR(base_lr=0.0001, max_lr=0.01,\n",
    "                        step_size=num_train_samples/train_batch_size_1/2)\n",
    "    csv_logger = CSVLogger(filename = base_path + \"\\\\model_history_{}_toplayer.csv\".format(model_name), append=True)\n",
    "    return [clr, earlystopping, ReduceLR, checkpoint, csv_logger]\n",
    "\n",
    "def get_callbacks(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=15, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    csv_logger = CSVLogger(filename = base_path + \"\\\\model_history_{}_ALL.csv\".format(modelname), append=True)\n",
    "    return [earlystopping, ReduceLR, checkpoint, csv_logger]\n",
    "\n",
    "name_weights = (base_path + \"\\\\{}_toplayer.h5\".format(model_name))\n",
    "callbacks_list = get_callbacks_clr(name_weights = name_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "#with tf.device('/device:CPU:0'):\n",
    "history_top_model_ttr=top_model_ttr.fit(trn_features, trn_labels, epochs=50, \n",
    "                                        batch_size=batch_size, validation_data=(val_features, val_labels),\n",
    "                                        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model_ttr.save_weights(model_path+'top_model_ttr_vgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_1 = datagen_train.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=train_batch_size_1,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "\n",
    "val_gen = datagen_val.flow_from_directory(val_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "nb_train_samples = len(train_gen_1)\n",
    "nb_validation_samples = len(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in top_model_ttr.layers: \n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_VGG(model_input):\n",
    "    base_model_VGG = applications.VGG16(weights='imagenet', include_top=False, input_tensor=model_input, input_shape=input_shape)\n",
    "    \n",
    "    for layer in base_model_VGG.layers:\n",
    "        layer.trainable = False\n",
    "        print(\"trainable:\", layer.name)\n",
    "    \n",
    "    x = base_model_VGG.output\n",
    "    x = GlobalMaxPooling2D(name='1')(x)\n",
    "    x = BatchNormalization(name='5')(out)\n",
    "    x = Dropout(0.5, name='6')(x)\n",
    "    x = Dense(512, activation='relu',name='7') (x)\n",
    "    x = BatchNormalization(name='8')(x)\n",
    "    x = Dropout(0.2, name='9')(x)\n",
    "    x = Dense(256, activation='relu',name='10') (x)\n",
    "    x = BatchNormalization(name='11')(x)\n",
    "    \n",
    "    predictions = (Dense(2, activation='softmax'))(x)\n",
    "    model_VGG = Model(inputs=inputs, outputs=predictions)\n",
    "    \n",
    "    return top_model_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG = create_model_VGG(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xception.load_weights(model_path+'top_model_ttr_vgg.h5',by_name=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "model_VGG.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\eg38emed\\FCD\\FCD vs TSC\\keras_utils\")\n",
    "from keras_lr_finder import LRFinder\n",
    "\n",
    "lr_gen = datagen_train.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=5000,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "x_lr_train,y_lr_train= lr_gen.next();x_lr_train.shape\n",
    "lr_finder = LRFinder(model_VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder.find(x_lr_train, y_lr_train, 0.000001, 0.1, batch_size=64, epochs=4)\n",
    "lr_finder.plot_loss(n_skip_beginning=1, n_skip_end=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clr_callback\n",
    "def get_callbacks_clr(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=10, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    clr = clr_callback.CyclicLR(base_lr=0.0001, max_lr=0.01,\n",
    "                        step_size=num_train_samples/train_batch_size_1/2)\n",
    "    csv_logger = CSVLogger(filename = base_path + \"\\\\model_history_{}_toplayer.csv\".format(model_name), append=True)\n",
    "    return [clr, earlystopping, ReduceLR, checkpoint, csv_logger]\n",
    "\n",
    "def get_callbacks(name_weights):\n",
    "    ReduceLR = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, verbose=1, mode='auto', cooldown=5, min_lr=0.00005)\n",
    "    earlystopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=15, verbose=1, mode='auto')\n",
    "    checkpoint = ModelCheckpoint(name_weights, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "    csv_logger = CSVLogger(filename = base_path + \"\\\\model_history_{}_ALL.csv\".format(modelname), append=True)\n",
    "    return [earlystopping, ReduceLR, checkpoint, csv_logger]\n",
    "\n",
    "name_weights = (base_path + \"\\\\{}_toplayer.h5\".format(model_name))\n",
    "callbacks_list = get_callbacks_clr(name_weights = name_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_VGG(model_input):\n",
    "    base_model_VGG = applications.VGG16(weights='imagenet', include_top=False, input_tensor=model_input, input_shape=input_shape)\n",
    "    \n",
    "    for layer in base_model_VGG.layers:\n",
    "        layer.trainable = True\n",
    "        print(\"trainable:\", layer.name)\n",
    "    \n",
    "    x = base_model_VGG.output\n",
    "    x = GlobalMaxPooling2D(name='1')(x)\n",
    "    x = BatchNormalization(name='5')(out)\n",
    "    x = Dropout(0.5, name='6')(x)\n",
    "    x = Dense(512, activation='relu',name='7') (x)\n",
    "    x = BatchNormalization(name='8')(x)\n",
    "    x = Dropout(0.2, name='9')(x)\n",
    "    x = Dense(256, activation='relu',name='10') (x)\n",
    "    x = BatchNormalization(name='11')(x)\n",
    "    \n",
    "    predictions = (Dense(2, activation='softmax'))(x)\n",
    "    model_VGG = Model(inputs=inputs, outputs=predictions)\n",
    "    \n",
    "    return top_model_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning Python 3",
   "language": "python",
   "name": "deep_learning_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
